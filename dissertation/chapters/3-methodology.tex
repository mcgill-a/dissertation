\chapter{Methodology}
This chapter will provide an overview of the data collection, augmentation, and pre-processing steps (Section \ref{sec:3-data}). It will cover details of the \acrshort{NMT} architecture and training methodology (Section \ref{sec:3-model}), before explaining the transfer learning implementation (Section \ref{sec:3-transfer_learning}) and translation evaluation metrics (Section \ref{sec:3-evaluating}).
\newpage

\section{Data}
\label{sec:3-data}
\subsection{Available Datasets}

The datasets that have been retrieved for use in the translation models have been split into three broad categories to reflect the type of vocabulary and sentence structure that can be expected from each dataset. They are described as follows:

\begin{itemize}
    \item Parliament - Publications of official parliamentary proceedings
    \item Technical - Technical software localisation files
    \item Informal - Informal conversation excerpts and natural sentences
\end{itemize}

A description of each dataset is shown in Table \ref{tab:available-data}, where English = EN, Irish Gaelic = GA, Scottish Gaelic = GD, French = FR, German = DEU, Italian = ITA, and Spanish = SPA.

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Languages}} & \multicolumn{1}{c|}{\textbf{Sentences}} & \textbf{Category} & \multicolumn{1}{c|}{\textbf{Source}} \\ \hline
EN \textrightarrow \space FR   & 2,000,000     & Parliament    & Europarl (\cite{french_corpus_2005}) \\ \hline
EN \textrightarrow \space GA   & 521,000       & Parliament    & ParaCrawl Corpus (\cite{irish_paracrawl_2020}) \\ \hline
EN \textrightarrow \space FR   & 170,000       & Informal      & Tatoeba French (\cite{tatoeba_data_2020}) \\ \hline
%EN \textrightarrow \space FR   & 137,500       & Informal      & Udacity Language Translation Dataset (\cite{udacity_data_2020}) \\ \hline
EN \textrightarrow \space DEU  & 115,000       & Informal      & Tatoeba German (\cite{tatoeba_data_2020}) \\ \hline
EN \textrightarrow \space GA   & 98,000        & Parliament    & Irish Legislation (\cite{irish_corpus_2017}) \\ \hline
EN \textrightarrow \space ITA  & 90,000        & Informal      & Tatoeba Italian (\cite{tatoeba_data_2020}) \\ \hline
EN \textrightarrow \space SPA  & 80,000        & Informal      & Tatoeba Spanish (\cite{tatoeba_data_2020}) \\ \hline
EN \textrightarrow \space GD   & 57,500        & Technical     & OPUS: GNOME v1 (\cite{tiedemann_opus_2012}) \\ \hline
EN \textrightarrow \space GD   & 36,500        & Technical     & OPUS: Ubuntu v14.10 (\cite{tiedemann_opus_2012}) \\ \hline
EN \textrightarrow \space FIN  & 33,000        & Informal      & Tatoeba Finnish (\cite{tatoeba_data_2020}) \\ \hline
EN \textrightarrow \space GA   & 1,900         & Informal      & Tatoeba Irish (\cite{tatoeba_data_2020}) \\ \hline
EN \textrightarrow \space GD   & 1,800         & Informal      & LearnGaelic PDF Materials (\cite{learn_gaelic_2019}) \\ \hline
%EN \textrightarrow \space GD   & 1,300         & Informal      & OPUS: Bible (\cite{bible_corpus_2015}) \\ \hline
EN \textrightarrow \space GA   & 900           & Informal      & Tatoeba Gaelic (\cite{tatoeba_data_2020}) \\ \hline
\end{tabular}
\captionsetup{justification=centering}
\caption{\label{tab:available-data} Description of available data}
\end{table}

\subsection{Augmentation}
% Write about back-translating some of the Irish data into Scottish Gaelic data

% edit this section so it matches new updates regarding generated data
The back-translation data augmentation technique identified in the literature in Section \ref{sec:2-low_resource_mt} has been used to generate additional Scottish Gaelic training data. The similarities between Irish Gaelic and Scottish Gaelic make it an ideal candidate for back-translation. Using the Irish Legislation corpus (\cite{irish_corpus_2017}) and Tatoeba Irish (\cite{tiedemann_opus_2012}) as reference material, an additional 100,000 Scottish Gaelic parallel training samples were generated. This was achieved by importing the parallel dataset into Google Sheets (\cite{google_sheets_2020}) and using the formula integration with Google Translate to bulk translate the entire dataset.


Sentence samples from table \ref{tab:back_translated-data} show that despite minor differences in word choice and order, the back-translated data retains the meaning of the original sentence. The ``Gaelic Translated" field is the original Irish Gaelic data translated into Scottish Gaelic. The ``English Translated" field can be used to compare the English sentences as it represents the Scottish Gaelic translation translated again back into English. It is worth noting that the supplementary English translation is unlikely to retain the same level of quality in comparison to the Scottish Gaelic translation, given the relatedness of the language pair and anticipated degradation through a translation of a translation.

\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|l|}{\textbf{Example 1}}                              \\ \hline
Original English           & I have to go to bed.                     \\ \hline
Original Irish Gaelic      & Caithfidh mé dul a chodladh.             \\ \hline
Translated Scottish Gaelic & Feumaidh mi a dhol dhan leabaidh.        \\ \hline
Translated English         & I must go to bed.                        \\ \hline
\multicolumn{2}{|l|}{\textbf{Example 2}}                              \\ \hline
Original English           & The lion is the king of the jungle.      \\ \hline
Original Irish Gaelic      & Is é an leon rí na dufaire.              \\ \hline
Translated Scottish Gaelic & Tha an leòmhann a tha an rìgh an Jungle. \\ \hline
Translated English         & The lion is the king of the Jungle.      \\ \hline
\end{tabular}
\captionsetup{justification=centering}
\caption{Back-translated data augmentation}
\label{tab:back_translated-data}
\end{table}

A minor variation of this technique has been replicated on multiple Tatoeba datasets, extracting the English data from a parallel corpus to create a monolingual corpus and using the Google Translate API to generate additional parallel data in Scottish Gaelic. Although this technique is unable to retain 100\% accuracy for the majority of cases, the impact that the additional data has on the quality of the \acrshort{NMT} translation in comparison to a very limited dataset is significant. These changes should not affect the BLEU score evaluation metrics as the output translation is compared with the original English sentence and not the augmented sentence.


\subsection{Analysis}

The data identified in Table \ref{tab:available-data} quantifies the difference between the high-resource languages such as French and Irish versus low-resource languages such as Scottish Gaelic. 

Although the size of a dataset may be large, limitations in the maximum length of a sentence mean that the number of sentences that meet the criteria for use in the neural network is often a significantly lower amount. The RNN accepts variable-length sequences using zero-padding to make them all the same length, meaning that all sentences match the length of the longest sentence. This may lead to issues where excessive padding on shorter sentences overwhelms the input sequence signal, making it difficult for the network to accurately decode an output sequence. As identified in research by \cite{cho_properties_2014}, the maximum sentence limit has been set to 20 to help prevent this from occurring.

While searching for different sources of data it became clear that parliamentary data is a popular source of parallel data due to the established guidelines of governments and the European Union where proceedings and legislation are required to be transcribed and translated into specific languages. As a result of this, Irish Gaelic data is abundant in this format. Despite the abundance of data, it was discovered that while parliamentary data may be useful for translation of other parliamentary transcripts, alternative datasets prove more beneficial for more general-purpose contexts. The parliamentary data consists of very long sentences that often consist of a lot of legal terminology relating to a specific piece of legislation regarding a place, organisation, references, and dates. 
As such, the vocabulary size is very large and the majority of the sentences exceed any reasonable threshold for sentence length as set by the parameters of the neural network. For example, despite having 2 million sentences in the \cite{french_corpus_2005} dataset, 1.38 million sentences exceed the maximum sentence length, drastically reducing the quantity of usable data. The sentence length distribution can be seen in Figure \ref{fig:sentence_length-french_legal}.

\begin{figure}[ht!]
\centering
\includegraphics[width=1\textwidth]{media/methodology/s_length-1-en_fr.jpg}
\captionsetup{justification=centering}
\caption[Europarl dataset sentence length distribution]{Sentence length distribution - Europarl English and French dataset} % (\cite{french_corpus_2005})
\label{fig:sentence_length-french_legal}
\end{figure}

In contrast, there is very little high quality parallel Scottish Gaelic data readily available. A large percentage of the original Scottish Gaelic data is technical information which contains a lot of software specific keywords, links and technical jargon. This is not ideal reference material for \acrshort{NMT} training data as it leads to a huge vocabulary size where the majority of words appear very few times and do not form coherent sentences, rather short descriptors of the field they represent in the respective localisation file. 

The LearnGaelic data was extracted from learning materials on the \cite{learn_gaelic_2019} website. PDFs are provided on a static template with the English text on one side and the Scottish Gaelic version of the same text on the other. Converting these PDFs into the HTML format allowed the data to be categorised and extracted into individual text files while retaining the original alignment of sentences between English and Scottish Gaelic.
Despite the low quantity of data from the LearnGaelic source, this data could be considered the highest quality as it consists of a diverse set of conversations that are quite informal and natural. Similarly, the original Tatoeba datasets consist of concise, natural sentences that have been manually translated and aligned by an online community of translators.

The composition of the data corpus can have a significant impact on the results of the translation quality evaluation. Should the entire training corpus be of a similar context, structure, and limited vocabulary, evaluations on the test set can be inflated due to their similarity with the training set.
To create a varied data corpus in the low-resource context, a subset of 145,000 samples that do not exceed the maximum sentence length from the multiple datasets have been selected to construct the core dataset for the \acrshort{NMT} models. The sources selected for this dataset can be seen in Table \ref{tab:low_resource-data}.

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Quantity}} & \multicolumn{1}{|c|}{\textbf{Source}} \\ \hline
109,000  & Tatoeba German (\cite{tatoeba_data_2020}) \\ \hline % 113,500
%50,000  & Udacity French (\cite{udacity_data_2020}) \\ \hline % 104,000
32,500  & Tatoeba Finnish (\cite{tatoeba_data_2020}) \\ \hline % 32,950
1,800   & Tatoeba Irish (\cite{tatoeba_data_2020}) \\ \hline
900     & Tatoeba Gaelic (\cite{tatoeba_data_2020}) \\ \hline
800     & LearnGaelic (\cite{learn_gaelic_2019}) \\ \hline
\end{tabular}
\captionsetup{justification=centering}
\caption{Final Scottish Gaelic dataset (original + back-translated)}
\label{tab:low_resource-data}
\end{table}

The sentence length distribution of the final Scottish Gaelic dataset is shown in Figure \ref{fig:sentence_length-gaelic}.
\begin{figure}[ht!]
\centering
\includegraphics[width=1\textwidth]{media/methodology/s_length-2-en_gd.jpg}
\captionsetup{justification=centering}
\caption[Scottish Gaelic dataset sentence length distribution]{Sentence length distribution - English and Gaelic dataset}
\label{fig:sentence_length-gaelic}
\end{figure}


\subsection{Pre-processing}

As outlined in the Tensorflow \acrshort{NMT} documentation (\cite{tensorflow_preprocess_2020}), a series of data cleaning and processing is required to ensure consistency throughout the dataset. Subsequently, tokenization will convert the data into sequences of word indices that can be understood by the \acrshort{NMT} models. This includes the following steps:

Data Cleaning:
\begin{itemize}
    \item Convert all characters to lowercase and from Unicode to ASCII. % Why ASCII???
    \item Replace all characters outwith [a-z, ".", "?", "!", ","]
    \item Insert a space between words and punctuation
    \item Truncate consecutive character spacing
    \item Exclude sentence pairs that exceed the maximum word limit
\end{itemize}

Data Processing:
\begin{itemize}
    \item Add <start> and <end> string delimiters to target sentences
    \item Enforce the vocabulary limit, prioritising most frequent words
    \item Word replacement for out of vocabulary or below minimum occurrence threshold
    \item Tokenize the source and target sentences using their vocabularies
\end{itemize}

\section{Model}
\label{sec:3-model}

\subsection{Architecture}
The full model consists of a source input layer, encoder \acrshort{GRU}, attention layer, target input layer, decoder \acrshort{GRU}, concatenate layer, and time distributed dense layer.
The model uses a Bahdanau attention layer, where the encoder and decoder hidden states are searched for the most relevant information. The difference between an attention layer architecture implementation using \cite{bahdanau_neural_2016} and \cite{luong_effective_2015} is that the output tensor from Luong attention is used as an additional input to the decoder \acrshort{GRU}, whereas Bahdanau attention receives output tensors from both the encoder \acrshort{GRU} and decoder \acrshort{GRU}.
The concatenate layer receives input in the form of tensors from the decoder \acrshort{GRU} and attention layer, concatenating them and outputting a single tensor. Finally, the time distributed dense layer applies the fully connected dense layer to every timestep in the \acrshort{GRU}. The full model architecture is shown in Figure \ref{fig:model_diagram}.



\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\textwidth]{media/methodology/model_architecture.jpeg}
\captionsetup{justification=centering}
\caption[Model architecture]{Model architecture}
\label{fig:model_diagram}
\end{figure}

Keras (\cite{keras_2015}) was used to implement the proposed model architecture along with a variety of other machine learning specific tasks such as padding variable-length sequences, one-hot encoding, and the training and inference methods described in subsequent sections.

The internal structure of the model is dependent on a set of parameters that are defined during the model initialisation. 
Tuning of these parameters can have a big impact on the translation model efficiency, speed, and quality. The parameters for this project are stored in a separate parameters file (Appendix \ref{AppendixD}) that is linked to the runtime of experiments to store the relevant state of the parameters per experiment. The key parameters that are available for tuning are shown in Table \ref{tab:model-parameters} along with a description of their purpose and their default value.


\begin{table}[!ht]
\centering
\small
\begin{tabular}{|l|p{6.6cm}|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Parameter}} & \multicolumn{1}{c|}{\textbf{Description}}                                       & \textbf{Default} \\ \hline
BATCH\_SIZE                & Batch size of the \acrshort{GRU} inputs            & 64    \\ \hline
HIDDEN\_UNITS              & Number of hidden units in the \acrshort{GRU}       & 128   \\ \hline
LEARNING\_RATE             & Learning rate of the optimiser                     & 0.001 \\ \hline
DROPOUT\_W                 & Dropout rate                                       & 0.2   \\ \hline
DROPOUT\_U                 & Recurrent dropout rate                             & 0.2   \\ \hline
SOURCE\_TIMESTEPS          & Number of timesteps for the source language        & 20    \\ \hline
TARGET\_TIMESTEPS          & Number of timesteps for the target language        & 20    \\ \hline
TEST\_SPLIT                & Training / test data split                         & 0.2   \\ \hline
VALIDATION\_SPLIT          & Training / validation data split                   & 0.2   \\ \hline
MAX\_WORDS\_PER\_SENTENCE  & Maximum number of words in a sentence              & 20    \\ \hline
MIN\_WORD\_OCCURRENCE      & Minimum number of occurrences to be included in vocabulary & 5 \\ \hline
FORCE\_SOURCE\_VOCAB\_SIZE & Source language vocabulary size limit              & 5000 \\ \hline
FORCE\_TARGET\_VOCAB\_SIZE & Target language vocabulary size limit              & 5000 \\ \hline
\end{tabular}
\captionsetup{justification=centering}
\caption{Key model parameters}
\label{tab:model-parameters}
\end{table}


\subsection{Training}
Training neural networks on Tensorflow is possible on both a GPU and a CPU. Due to fundamental hardware differences between the two such as memory bandwidth and memory access latency, using a GPU can accelerate the training of models with large datasets. To take advantage of the increased computational efficiency and reduction in training time, all of the training conducted during experimentation was done using Tensorflow GPU.

Given the hardware constraints of a GTX 980 4GB graphics card, the model has a batch size of 64 and has 128 hidden units. This ensures that the video memory resources will not be exhausted and interrupt training. Had more resources been available, the parameters identified in the transfer learning literature would be replicated (256 batch size and 1000 hidden units). This would significantly increase the number of trainable parameters which means that the network has more flexibility in representing the desired mapping.

Before training takes place, the data is split up into training data and testing data, with 80\% of the data to be assigned to training and the remaining 20\% kept separate for the test set which will be used for evaluation. The training data is split again into the final training set (80\%) and validation set (20\%).

The models are trained using the Adam optimizer (\cite{adam_optimizer_2014}) with a learning rate of 0.001 and the categorical cross-entropy loss function. To help prevent overfitting during early epochs and improve generalisation of the models, dropout and recurrent dropout can be applied to the encoder \acrshort{GRU} and decoder \acrshort{GRU} with a value of $0.2$. 

The full model, encoder model, and decoder model are all saved at the end of an epoch if the mean validation loss of the epoch improves upon the previous best validation loss. If no improvements are observed after 5 epochs then training is stopped.


Along with the full model, an inference encoder and decoder model are defined during initialisation. The inference models are used to predict the translation of source sentences into the target language. Separate models are required because the full model expects input from the source and target language. In contrast, during inference, a single input from the source language is received and output in the target language is inferred. As well as the output translation, attention weights are also saved during inference.

\section{Transfer Learning}
\label{sec:3-transfer_learning}

The experiments investigate the use of both trivial transfer learning and hierarchical transfer learning to take advantage of the knowledge gained on a high-resource language, initialising the low-resource language to improve translation quality as identified in the literature review (Section \ref{sec:2-transfer_learning}).

The high-resource languages that have been selected for use in transfer learning are French and Irish Gaelic. In theory, the use of Irish Gaelic as an intermediary language for hierarchical transfer learning should help transform the French word embeddings closer to a representation that better fits the syntactic structure of Scottish Gaelic.


When a model is defined, the vocabulary size of the source language and the target language is used as a parameter to define the shape of encoder and decoder \acrshort{GRU}. High-resource languages will typically have a much higher vocabulary size by default given that there are many more training examples where unique words are likely to occur. A vocabulary size limit is required for transfer learning as the input shape that is passed to the encoder and decoder must be the same as the initial declaration. The restriction on vocabulary means that these sizes remain consistent between different languages and datasets.

The vocabulary size has been restricted to at most 7,000 for all languages used in the experiments, replacing words outside of this limit with the out of vocabulary unknown word token ``UNK". The implementation of this restriction prioritises the most frequently occurring words by sorting them by frequency in descending order and adding up to 7,000 words to the vocabulary dictionary.
In addition to the vocabulary size limit, a minimum word replacement can be specified to remove words that only occur in a very small subset of training samples. Given the limited vocabulary size present in the low-resource language training dataset, this value remains at 2 for the duration of the experiments that use the Scottish Gaelic data. Despite a minimum word replacement value of 2 for the high-resource languages, most of the training samples will significantly exceed this value as a result of the limited vocabulary size and prioritisation of more important words.

\section{Evaluating Translations}
\label{sec:3-evaluating}

As identified in the literature review, \acrshort{BLEU} scores will be used as the primary metric for the translation evaluation. To ensure the robustness of the results, evaluations will be presented in the form of \acrshort{BLEU}-1 to \acrshort{BLEU}-4. Although a translation may receive a high score for \acrshort{BLEU}-1, \acrshort{BLEU}-4 scores are often considerably lower due to the higher percentage of n-gram counts that are required to match the reference translation. The evaluations are calculated using a cumulative score rather than the individual score as it better represents the metric distribution. Additionally, the \acrshort{NIST} evaluation metric will be used alongside the \acrshort{BLEU} score on the final three models to improve the validity of the translation evaluations. The final form of evaluation will be a sentence analysis table. The direct comparison between implementation translations makes it easier to understand the discrepancies outlined by \acrshort{BLEU} scores.