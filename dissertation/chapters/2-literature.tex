\begin{savequote}[75mm]
Add your quote here
\qauthor{Author of quote}
\end{savequote}

\chapter{Literature Review}
\label{literature}
\newthought{Lorem ipsum dolor sit amet}, consectetur adipiscing elit. Nunc neque massa, eleifend malesuada enim eu, efficitur imperdiet nulla. Aliquam augue est, lobortis eget scelerisque at, pretium vel lorem. Etiam faucibus nulla quam, nec tristique elit ullamcorper quis. Phasellus vel tincidunt nisl. Mauris at lacus sit amet mi auctor feugiat. Donec vel maximus erat. Suspendisse feugiat augue lacus, non egestas libero auctor sit amet. Nunc eros ante, posuere quis justo id, congue ultrices sapien.

\clearpage


\section{Machine Learning}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.

\subsection{Neural Networks}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.

%Vivamus placerat tortor a egestas sollicitudin. Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet. Fusce lacinia justo dui, ut ornare tortor pharetra consequat.

\subsection{Deep Learning}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.

%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc neque massa, eleifend malesuada enim eu, efficitur imperdiet nulla. Aliquam augue est, lobortis eget scelerisque at, pretium vel lorem\footnote[2]{Vivamus placerat tortor a egestas sollicitudin. Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut.}. Etiam faucibus nulla quam, nec tristique elit ullamcorper quis. Phasellus vel tincidunt nisl. Mauris at lacus sit amet mi auctor feugiat. Donec vel maximus erat. Suspendisse feugiat augue lacus, non egestas libero auctor sit amet. Nunc eros ante, posuere quis justo id, congue ultrices sapien.


\subsection{Keras}
%Vivamus placerat tortor a egestas sollicitudin. Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet. Fusce lacinia justo dui, ut ornare tortor pharetra consequat.



\section{Machine Translation}
\label{Machine Translation}


\subsection{Training Data}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.

\subsection{Techniques}

\subsubsection{Rule-Based Machine Translation}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.

\subsubsection{Statistical Machine Translation}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.

\subsubsection{Transfer-Based Machine Translation}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.


\subsubsection{Neural Machine Translation}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.

\subsection{Evaluation}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.


\section{Low-Resource Neural Machine Translation Approaches}
\label{NMT}

\subsection{Transfer Learning}

As outlined by \cite{torrey_transfer_2009}, transfer learning is able to use the knowledge gained from a previous task in order to improve model performance in a related task.
In a neural machine translation context, this involves training a model using data from a high-resource language and then using that model for initialising the model the will be trained on the low-resource language training data. This was demonstrated in research carried out by \cite{zoph_transfer_2016}, where transfer learning improved the performance of NMT models for low-resource languages by an average of 5.6 \acrshort{BLEU} on four different language pairs. Results of the experiment also suggest that selecting a high-resource language closely related to the low-resource language can improve transfer learning models and therefore translation quality. However, this contradicts more recent research by \cite{kocmi_trivial_2018}, which looked at "trivial transfer learning". Whereas existing transfer learning methods require a degree of language relatedness, trivial transfer learning prioritises data quantity for the high-resource language. Their findings indicate that the relatedness of the language pair is of less importance than the quantity of data used in the initial high-resource language training. Despite being unable to pinpoint the exact reasoning behind the improvement in results, they state that "our observations indicate that the key factor is the size of the parent corpus rather than e.g. vocabulary overlaps."
It is worth noting that \cite{kocmi_trivial_2018} use a transformer neural network architecture instead of the recurrent neural network architecture used by \cite{zoph_transfer_2016}. Research by \cite{popel_training_2018} found that using the transformer model leads to better translation quality, likely contributing towards the contradictory results.


Hierarchical transfer learning seeks ensure the closeness of the related language pair, as identified in most transfer learning research, while simultaneously addressing the importance of the high-resource data quantity outlined in trivial transfer learning. \cite{luo_hierarchical_2019} manage to achieve this by implementing three distinct stages of training. The first stage involves training using an related high-resource language pair.

% Once the model has

% Train the NMT model using the unrelated high-resource language pair
% Train using the similar intermediate language pair
% Train using the low-resource language pair


\subsection{Meta Learning}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet.

\section{Conclusion}
%Nulla fringilla eu eros nec tempor. Phasellus volutpat nulla a lectus pulvinar, sit amet maximus ex semper. Fusce sagittis ligula magna, non condimentum lectus imperdiet ut. Nunc dignissim turpis nibh, vel tristique tellus tristique eu. Fusce posuere dictum ultricies. Nam a metus quis mauris gravida laoreet. 