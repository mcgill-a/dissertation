%\begin{savequote}[75mm]
%Add your quote here
%\qauthor{Author of quote}
%\end{savequote}

\chapter{Literature Review}
\label{literature}
This chapter will present the background information and approaches for Deep Learning (Section 2.1) and discuss the most prevalent techniques and evaluation methods in Machine Translation (Section 2.2). It will then investigate data augmentation in Low Resource Machine Translation (Section 2.3) and Low-Resource Machine Translation Approaches (Section 2.4). Finally, the findings will be concluded in the Literature Review Conclusion (Section 2.5).


\clearpage
\section{Deep Learning}
\label{section:deep_learning}
% Give an actual introduction here explaining basic perceptron and how CNNs can be used in NLP and translation

% What is a neural network?

%\cite{schmidhuber_deep_2015}


% What is deep learning?
Deep learning is a subset of machine learning inspired by the human brain that uses artificial neural networks with many hidden layers to extract features from inputs while training with large amounts of data. %The deep neural networks that are trained on a large parallel corpus are the 
Deep learning neural networks such as a \acrfull{CNN} expand upon the concept of feedforward neural networks like the perceptron. A single-layer perceptron is the most basic form of neural network that is used for binary classification, with a single layer of output nodes that are connected directly to weighted inputs through an activation function. A multi-layer perceptron expands the single-layer perceptron with the addition of hidden layers, where all nodes in one layer are connected to all the nodes in the next layer, as shown in Figure \ref{fig:perceptron_multi}. The additional layers allow the perceptron to solve nonlinear classification problems (\cite{driss_comparison_2017}).

\begin{figure}[ht!]
\centering
\includegraphics[width=0.65\textwidth]{media/literature/machine_learning/ml_perceptron_multi.png}
\caption[Diagram of a multi-layer perceptron]{Multi-Layer Perceptron (\cite{michael_perceptron_2019})}
\label{fig:perceptron_multi}
\end{figure}

%\cite{goodfellow_deep_learning_2016} talks about feedforward networks (MLPs)

Extracting simple features from the lower levels of representation helps to identify the abstract features present in the higher representation levels that lead to the output classification of data (\cite{bengio_deep_2011}).
%
The intricacies of a data structure are identified using backpropagation to determine how the neural network should update the weights that are responsible for calculating the representation in each layer, based on the representation of the previous layer. (\cite{lecun_deep_2015}). The proceeding chapters explore the literature surrounding deep learning neural networks, specifically the \acrfull{CNN}, \acrfull{RNN}, and \acrfull{GRU}.



% What are they good at doing?
% They are good at speech recognition, image recognition, natural language processing, facial detection, self driving, machine translation

% Introduce NMT

% \cite{lecun_deep_2015} DL background information. MLPs, CNNs, RNNs

%\subsection{Feedforward Neural Networks}
\subsection{Convolutional Neural Networks}

%\subsubsection{Introduction}
% what is a CNN?

A \acrfull{CNN} is an artificial neural network similar to the multi-layer perceptron with additional hidden convolutional layers. \acrshort{CNN}s are very good at detecting patterns from data which makes them ideal for image classification, object recognition, and more recently \acrfull{NLP} (\cite{young_cnns_recent_2018}).

Research by \cite{lecun_backprop_cnn_1989} was the first to demonstrate how the backpropagation algorithm proposed by \cite{rumelhart_learning_1986} could be integrated into a convolutional neural network.
Using the \acrshort{CNN}, they successfully performed character recognition and classification on images of handwritten digits from data provided by the U.S Postal Service.

The \acrshort{CNN} architecture is shown in Figure \ref{fig:cnn_1} using an example of image classification.

\begin{figure}[ht!]
\centering
\includegraphics[width=1\textwidth]{media/literature/machine_learning/ml_cnn_1.png}
\caption[Diagram of a convolutional neural network architecture]{Convolutional Neural Network architecture (\cite{lopez_deep_2017})}
\label{fig:cnn_1}
\end{figure}

A convolutional layer typically involves three different actions (\cite{goodfellow_deep_learning_2016}):
\begin{itemize}
    \item Run multiple convolutions in parallel, producing a set of linear activations
    %item The layer performs several convolutions in parallel to produce a set of linear activations
    \item Use a nonlinear activation function on each linear activation
    %\item Each linear activation is run through a nonlinear activation function such as \acrshort{RELU}
    \item Use a pooling function to downsample the layer output
    %\item Use a pooling function to modify the output of the layer further
\end{itemize}


% Convolutional layers + filters
A convolution is a mathematical operation that generates an activation map (matrix) using inputs such as an image matrix and a filter, outputting high values if the convolution feature is present in that location. For image classification, a filter represents a small matrix with a preset number of columns and rows.
A convolutional layer has a specified number of filters that are used to detect different patterns. These filters are initialised with random numbers and adjusted using backpropagation to learn the weights automatically.
%3x3 is a common matrix size because as the matrix size increases, so does the number of parameters, increasing the likelihood of overfitting. 
When a convolutional layer receives an input, the filter convolves over each $n$ x $n$ block of pixels in the image and the value of each cell becomes the dot product of the block of pixels and the filter. The resultant matrix is used as input to the next layer.

In early convolutional layers, filters may only detect very simple features such as edges and shapes but as the layers get deeper in the neural network, filters are able to identify more complicated objects such as a facial features or types of animals. These high level features are what the classifier uses for weighting the output predictions.
\acrshort{NLP} tasks consist of text input sentences rather than images so rows within a matrix are word embedding vectors that are generated using models such as word2vec (\cite{mikolov_word2vec_2013}). 
As each row in the matrix represents an entire word, filters span the full width of the row to match the width of the matrix input, with a height of between $2$ - $5$ words (\cite{lopez_deep_2017}).

% Activation function
The activation function of a neural network transforms the weighted input of a neuron into the activation of the output, determining whether a neuron fires or not. Unlike the sigmoid and hyperbolic tangent activation functions that suffer from the vanishing gradient problem, \acrfull{RELU} converges quickly and overcomes the vanishing gradient problem, making it the recommended activation function for modern \acrshort{CNN}s (\cite{nair_rectified_2010}). 
Leaky ReLU is a variation of \acrshort{RELU} that applies a small negative gradient when $x < 0$ to prevent the dying \acrshort{RELU} problem.

% Pooling layer
Feature maps (the output activations for a given filter) are sensitive to the position of a feature in an image. Pooling layers address this issue by reducing the resolution of the feature maps to introduce translation invariance (\cite{scherer_evaluation_2010}). The downsampled feature maps can be thought of as a summary of the nearby outputs present in small $n$ x $n$ patches (the pooling window) of the feature map. The most common methods of pooling are are max pooling and average pooling, however \cite{scherer_evaluation_2010} found that the max pooling operation significantly outperforms subsampling operations.

% Fully connected layers
The fully connected layers take outputs from the convolution and flatten them into a single vector. Using the Softmax activation function, the values of the vector in the final layer represent the probabilities of features matching the labels used for classification. Every neuron has full connections to all of the activations from the previous layer, and activations are computed using matrix multiplication and a bias offset (\cite{stanford_cs231n_2019}).

% Original:
%The fully connected layers take outputs from the convolution and flatten them into a single vector. The values of the vector represent the probabilities of features matching certain labels, subsequently used for classification. Every neuron has full connections to all of the activations from the previous layer, and activations are computed using matrix multiplication and a bias offset (\cite{stanford_cs231n_2019}).



\begin{figure}[ht!]
\centering
\includegraphics[width=0.30\textwidth]{media/literature/machine_learning/ml_pooling.png}
\caption[Diagram of CNN Max Pooling and Average Pooling ]{\acrshort{CNN} Max Pooling (MP) and Average Pooling (AP) (\cite{wang_pooling_2018})}
\label{fig:cnn_pooling}
\end{figure}

%These pooling methods are visualised in Figure \ref{fig:cnn_pooling}:
As demonstrated in Figure \ref{fig:cnn_pooling}, max pooling and average pooling are carried out by:

\begin{itemize}
    \item Max Pooling: Select the maximum value within the pooling window
    \item Average Pooling: Select the average value within the pooling window
\end{itemize}

% Dropout
Dropout is a technique that helps address the problem of overfitting for \acrshort{CNN}s that have many parameters. During training a random set of neurons and their subsequent connections in the network are dropped (ignored) with probability $p$ (\cite{srivastava_dropout_2014}). This can be seen in Figure \ref{fig:cnn_dropout}. In the original research by \cite{hinton_dropout_2012}, dropout was only applied to the fully connected layers of a \acrshort{CNN}. However, \cite{lai_dropout_2017}, has since found that regularisation increased when dropout is also used after the activation function of every convolutional layer in the network at a lower probability ($p = 0.1$).

\begin{figure}[ht!]
\centering
\includegraphics[width=0.55\textwidth]{media/literature/machine_learning/ml_dropout3.png}
\caption[Diagram of the Dropout Neural Network Model]{Dropout Neural Network Model. Left side: Standard \acrshort{NN} with 2 hidden layers. Right side: Standard \acrshort{NN} with 2 hidden layers, after applying dropout (\cite{srivastava_dropout_2014})}
\label{fig:cnn_dropout}
\end{figure}




% TODO
% - More information to introduce the CNN architecture
% - backpropagation (update weights) % REMOVE but mention
% XXX - Pooling layer (sub-sample input after convolutional layers)
% XXX - Activation function (decide if neuron fires or not) % mention briefly
% OOO - Batch normalisation (improve speed + performance, helps reduce overfitting)
% XXX - Dropout (ignore random set of neurons during training, helps reduce overfitting) 
% XXX - Fully connected layers
%    
%\end{itemize}


%\cite{lecun_deep_2015}

%\cite{jacovi_understanding_2018} conv nets for text classification: n-grams etc.


% Talk about CNNs for Neural Machine Translation

\subsection{Recurrent Neural Networks}

% Introduction
Traditional neural networks struggle to retain previous information as they only map input vectors to output vectors (\cite{graves_supervised_2012}). A \acrfull{RNN} is a type of neural network that performs the same function at every time step in a sequence, using the output of the previous step as the input to the current step.
Similar to the objectives of traditional neural networks such as a \acrshort{CNN}, the aim of an \acrshort{RNN} is to reduce the loss value between input and output pair predictions using backpropagation to learn the network weights. 
\acrshort{RNN}s are capable of mapping the entire history of previous inputs to each output, using internal hidden states to maintain a representation of the information that has been calculated previously and influence the network output.
% What are they good at?
This is particularly useful for \acrshort{NLP}, where the prediction of a word often depends on the context of what has been observed in the sentence so far. Short term memory also improves the handling of invariance for tasks such as image classification (\cite{mikolov_recurrent_slides_2010}), where the position, orientation, and size of an object may differ but the correct object is still identified. 

% Stolen. Could be modified and added somewhere though:
% \acrshort{RNN}s can handle much longer sequences than would be practical for networks without sequence-based specialisation \cite{\cite{goodfellow_deep_learning_2016}}




% Vanilla RNN (Equation) - formula for calculating the current state at every time step

The 'simple recurrent neural network' was first proposed by \cite{elman_original_rnn_1990} and consists of an input layer, a recurrent hidden layer, and an output layer. The recurrent hidden layer is essentially multiple copies of the same neural network that are connected sequentially and pass information forwards. The network can be 'unrolled' in a diagram to visualise the sequence, as shown in Figure \ref{fig:rnn_unrolled}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{media/literature/machine_learning/ml_rnn_unrolled.png}
\caption[Diagram of an unrolled Recurrent Neural Network]{Left side: A vanilla \acrlong{RNN}. Right side: An unrolled vanilla \acrlong{RNN} (\cite{colah_lstm_2019})}
\label{fig:rnn_unrolled}
\end{figure}

To process a sequence of vectors and calculate a new state, a recurrence formula is applied at each time step using a function of the previous state and the current input vector. Regardless of the input or output sequence length, the same function is used at each time step. This is shown below, where $h_t$ is the new state, $f_w$ is a function with parameters W (weights), $h_{t-1}$ is the previous state, and $x_t$ is the input vector at a given time step: 

\begin{equation}
    h_{t} = f_{w} \left ( h_{t-1},x_{t} \right )
\end{equation}

The \acrshort{RNN} receives an input vector every single time step and modifies the internal state. The weights inside the \acrshort{RNN} are used to determine the behaviour of how a state evolves when it receives an input. 
% =========================== Weights are updated using back propagation =========================== 
In a conventional neural network, information travels forwards through the input and output of neurons in the network. The loss value is calculated and then the weights are updated using backpropagation.
In an \acrshort{RNN}, the information output from previous time steps are used as input for future time steps and the loss value is calculated at each time step. In order to minimise the loss value, once the final loss function in the \acrshort{RNN} has been calculated, error signals need to be back-propagated using \acrfull{BPTT} through the entire network to update the weights of every neuron (\cite{salehinejad_recent_rnn_2018}).
% =========================== Then talk about vanishing and exploding gradient problems =========================== 
As outlined by \cite{bengio_learning_1994}, the further back an error signal is back-propagated, the harder it becomes for the network to update the weights. This is known as the vanishing gradient problem, where the gradient reduces such that the weights are essentially prevented from being updated, halting the training progress.
% =========================== Then talk about LSTM =========================== 

\subsubsection{\acrlong{LSTM}}

\acrfull{LSTM} is an \acrshort{RNN} architecture proposed initially by \cite{hochreiter_long_1997} that was designed to help overcome the vanishing gradient problem present during backpropagation. They are significantly better than simple \acrshort{RNN}s at capturing long term dependencies because they use a gradient-based algorithm that enforces a consistent internal state error flow. This ensures that gradients will not become insignificant and halt the learning process.

In the diagram shown in Figure \ref{fig:rnn_lstm}, an entire vector is carried through each line from the output of one node to the input of other nodes. The vectors go through three sigmoid ($\sigma$) gates and one tanh gate (learned neural network layers) to decide what inputs pass through the network and what gets blocked. The symbol in a pink circle denotes the pointwise operation applied to the vectors (addition or multiplication).

\begin{figure}[ht!]
\centering
\includegraphics[width=0.89\textwidth]{media/literature/machine_learning/ml_rnn_lstm.png}
\caption[Diagram of Long Short Term Memory]{\acrfull{LSTM} diagram (\cite{colah_lstm_2019})}
\label{fig:rnn_lstm}
\end{figure}

Predictions are based on the vectors that pass through the gates and a copy is kept for the next time step, meaning future predictions can be informed by memories that haven't forgotten. Referring back to previous time steps allows \acrshort{LSTM}s to better represent language specific grammar structures and transfer the meaning of sequences to other languages.


One of the issues of using \acrshort{LSTM}s comes from the bandwidth and memory resource constraints that are encountered in the architecture as a result of the high number of tensor operations required. A simpler architecture may be more computationally efficient at the risk of being less accurate for specific deep learning tasks.
% probably need to get a source for this

% What are the issues with LSTMs?

% hardware memory and bandwidth constraints - uses a lot of resources. Although this seems to be true for RNNs in general not just LSTMs. LSTMs might be worse though.

% same as LSTMs in that it was created as a solution to short term memory
% internal mechanisms called gates that can regulate the flow of info

% the gates learn which data in a sequence is important to keep or throw away
% pass relevant info down the chain of sequences to make predictions


% \cite{mikolov_recurrent_2010} paper (notes on notion) - learning long term dependencies by stochastic gradient descent difficult

% RNNs and LSTM
%For standard \acrshort{RNN} architectures, the 
%\cite{graves_supervised_2012}

% LSTM references:
% %\cite{graves_supervised_2012} - page 43
% \cite{goodfellow_deep_learning_2016} chapter 10 on neural networks

% 

% output of a particular layer is saved and fed back to the input
% this helps predict the outcome of the layer

% first layer is formed the same way as it is in the feed-forward network.  Product of the sum of the weights and features
% however, subsequent layers, the recurrent neural network process begins

% =============== CNN vs LSTM for sequence to sequence ===============

An encoder is a network that maps an input to a feature vector and a decoder is a network that transforms the encoder output into a sequence in the target language. Typically, sequence to sequence problems are solved by stacking both the encoder and decoder with layers of an RNN with \acrshort{LSTM} (\cite{luong_effective_2015}). However, \cite{gehring_convolutional_2017} proposes the first fully connected \acrshort{CNN} for sequence to sequence learning that outperforms high performance \acrshort{LSTM} translation models by up to $1.9$ \acrshort{BLEU}. This approach discovers more compositional structure than \acrshort{RNN}s due to the hierarchical representations of sequences.

\subsubsection{\acrlong{GRU}}

% =============== GRU ===============

% Write about GRUs
% How is it different to an LSTM?
% What are the benefits of GRUs?
% What are the drawbacks of GRUs?

\acrfull{GRU} is an architecture proposed by \cite{cho_properties_2014} that consists of an update gate, reset gate, and hidden state. Similar to an \acrshort{LSTM}, the aim of a \acrshort{GRU} is to help overcome the issue of short term memory in \acrshort{RNN}s.
The internal cell state in an \acrshort{LSTM} is not present in a \acrshort{GRU}. Instead, the information is represented in the hidden state vector which is passed onto subsequent \acrshort{GRU}s.
The update gate combines the input and forget gates and is used to determine what information should be learned and discarding the rest, whereas the reset gate decides which information to remove from memory (\cite{gao_gru_2016}).
A diagram of the described architecture can be visualised in more detail in Figure \ref{fig:rnn_gru}.
\begin{figure}[ht!]
\centering
\includegraphics[width=0.89\textwidth]{media/literature/machine_learning/ml_rnn_gru.png}
\caption[Diagram of Gated Recurrent Unit]{\acrfull{GRU} diagram (\cite{colah_lstm_2019})}
\label{fig:rnn_gru}
\end{figure}


As a result of these differences, \acrshort{GRU}s include fewer tensor operations which can lead to efficient computation and therefore a reduction in training time (\cite{chung_gru_2014}).
The performance of \acrshort{GRU} models favour short sentences without unknown words, degrading significantly as the length of the sentence and unknown word count increases (\cite{cho_properties_2014}).


\section{Machine Translation}
\label{Machine Translation}

This section will introduce machine translation in a review of two of the most prevalent techniques and a review of the literature surrounding metrics for automatic translation evaluations. Subsequent sections will focus on low-resource approaches to \acrshort{NMT} in relation to data augmentation and model architectures.

\subsection{Techniques}

%\subsubsection{Statistical Machine Translation}

% every sentence in one language is a possible translation of any sentence in another
% Assign every pair of sentences a probability
% Pr(T|S) - probability that a translator will produce T in the
%           target language when presented with S in the source language

% View the problem of machine translation as follows:
% given a sentence T in the target language,
% Seek the sentence S from which the translator produced T.
% Chance of error is minimised by choosing that sentence S that is most probable given T
% Thus, we wish to choose S so as to maximise Pr(S|T)

\acrfull{SMT} is a statistical approach for machine translation first presented by \cite{brown_statistical_1990}. 
\acrshort{SMT} assumes that all sentences in a source language have the possibility of being the correct translation of a sentence in a target language. 
In other words, find the source sentence $S$ that the translator used to produce the target sentence $T$ by selecting the pair with the highest probability $Pr ( T | S )$.
This is done using a language model, translation model, and decoder as shown in Figure \ref{fig:smt_diagram}.

\begin{figure}[ht!]
\centering
\includegraphics[width=1\textwidth]{media/literature/machine_translation/smt_3.png}
\caption[Diagram of \acrshort{SMT} probability distribution and decoder]{Left side: \acrshort{SMT} probability distribution for source ($S$) and target ($T$) sentence pairs. Right side: \acrshort{SMT} decoder selecting the highest probability sentence pair (\cite{brown_statistical_1990})}
\label{fig:smt_diagram}
\end{figure}



% SMT - instead of defining all the language rules and exceptions to rules, let the system find the rules itself
% analyse millions of parallel data sets that have already been translated by human translators
% once the computer finds the pattern, use this pattern to translate text in the future
% repeat this process a lot, lots of patterns, lots of translations


% probabilistic models estimated from parallel corpora
% need large quantities of training data


% From SLIDES: http://folk.uio.no/plison/pdfs/talks/meetup_SMT.pdf
% encodes the faithfulness of T as a translation of S

% early translation models relied on probabilities for individual words
% did not account well for idiosyncratic expressions

% decompose translation probability P(s|t)  into phrase pairs
% - phrase probability (as given by the translation table)
% - distortion probability (relative distance between the phrase positions in the two languages)

% Three challenges:
% estimating the language model probability, Pr(e) - called the language modelling problem
% estimating the translation model probability, Pr(f|e) - called the translation modelling problem
% devising an effective and efficient sub-optimal search for the English string that maximises their product
% ^ called the search problem search problem

%Explain the language model:
The language model provides an estimation of how probable a given source sentence is based on the probability distribution of word sequences. This model is associated with the fluency of the translation as it is trained using target language monolingual data, providing the guidelines for well written translation output. N-grams are all of the combinations of $n$ contiguous words that can be found in a source text. Using a large training corpus, the n-gram probability of every word is determined from the words immediately proceeding it. One drawback to this approach is that the unrecognised increased probability for words that are dependant on each other but are further apart.

%in scenarios where words are linked to other words that are further away from them, 

%n-gram probabilities on large amounts of monolingual data 

%This is accomplished by using large amounts of monolingual data, n-gram probabilities can 



% bigram or general m-gram model

% encodes the fluency of T in the target language

% also want the translated sentence T to be fluent in the target language
% statistical language model is a probability distribution over sequences of words
% typically represented as n-grams, where the probability of each word depends on the words occurring before it

% N-gram probabilities can be estimated from large amounts of monolingual data
% Bigrams or trigrams are most popular
% Smoothing methods to account for data sparsity

% Does not do well with long-range dependencies (words that depend on another word that is multiple words away)

%Explain the translation model:
% decomposed into a lexical and an alignment model
% estimation of the lexical correspondence between languages (lexical = words or vocab of a language)

%The translation model proposed by \cite{brown_statistical_1990} includes the following parameters:
The translation model is an estimation of the source and target language vocabulary correspondence. It is associated with the quality of the translations, as it is responsible for predicting the translations of words and short sequences of words, mapping the source language to the target language. Model parameters are estimated by training probabilistic models on large quantities of parallel training data, derived from the following sets of probabilities as proposed by \cite{brown_statistical_1990}:
\begin{itemize}
    \item Fertility probabilities:  $Pr(n|s)$ - the probability that a source word $s$ generates $n$ target words in a given source-target sentence alignment
    
    \item Lexical probabilities: $Pr(s|t)$ - the probability that a source word $s$ translates into a target word $t$, for each element in the source and target language vocabularies
    
    \item Distortion probabilities: $Pr(i|j, l)$ - the probability of the position of a target word $i$, based on the position of the source word $j$, and the length of the target sentence $l$
    
    %\item Probability p1: p(you|NULL) the probability that the spurious word you is generated from NULL
    % %\item if an English word isn't aligned with a french word then it is omitted
    %\item Lexical probability t: t(Quan|When) the probability that Quan translates into When
    
    %\item Distortion d: d(j|i, m, n) the probability that the word in position j generates a word in position i. m and n are the length of the source and target sentences
    % \item distortions allow adjectives to precede the noun that they modify in english but to follow them in french
\end{itemize}


% \cite{brown_mathematics_1993}
% the language model probability is large for well-formed English strings regardless of their connection to French.
% together, they produce a large probability for well-formed English strings that account well for French.

% alignment mapping i j -> i = a_j from source position j to target position i = a_j
% use of alignment model raises major problems as it fails to capture dependencies between groups of words
% it is difficult to handle different word order and the translation of compound nouns

% \cite{och_1999_improved} described two methods for SMT that extend the baseline alignment model
% to account for these problems


%Explain the decoder

During the runtime of \acrshort{SMT} systems, the decoder uses the translation and language models to find the best translation from the source input to the target language output. 
The decoder starts off with an empty hypothesis for the translation. The hypothesis is expanded incrementally by partial hypotheses using the translation and language models. As there is an exponential number of hypotheses in relation to the length of the source sentence, search optimisation techniques are required to find the most likely translation. A beam search is an optimised breadth-first search algorithm that searches the most promising nodes, storing and expanding only a limited number of states. In this scenario, it can be used as a decoding technique to confine the search space to a limited quantity of low cost hypotheses by comparing hypotheses with equal length translation output and removing those with a high cost and estimated future cost (\cite{koehn_pharaoh_2004}).

% how to use translation and language models to find the best translation T for a sentence S?

% Search through the space of possible translations
% incremental decoding process (beam search): gradual expansion of translation hypothesis
% every partial hypothesis is associated with a cost
% translation and language models + estimate of future costs
% beam search only keeps track of a limited number of good hypotheses (based on their cost), the rest is discarded

% the search space is further reduced through hypothesis recombination


%\cite{brown_statistical_1990}
%\begin{itemize}
%    \item SMT first introduced
%    \item Most successful phrase-based method to translate input text
%    \item Translate sequences of words at a time using a parallel corpus
%\end{itemize}


%\cite{schwenk_continuous_2012}
%\begin{itemize}
%    \item Phrase-based SMT further improvements
%\end{itemize}

% introduced by IBM 1990, 1993
% most successful use phrase-based method to translate input text
% by translating sequences of words at a time

% phrase based use a language model trained from a parallel corpus
% translation model in broken down into several components:
% reordering model, phrase translation model, world translation model

\subsubsection{Neural Machine Translation}

\acrfull{NMT} is a modern approach to machine translation that uses neural networks to generate statistical models capable of translating sentences from a source language into sentences in the target language.
% mention its wide spread adoption and state of the art results
% used by companies such as Google for Google Translate
% Bahdanau has a background section on NMT:
% in NMT, we fit a parameterized model to maximise the conditional probability of sentence pairs
% using a parallel training corpus
% Once the conditional distribution is learned by a translation model
% given a source sentence a corresponding translation can be generated by searching for
% the sentence that matches the conditional probability
%<<introduce sequence to sequence models, leading onto the next section>>
%continuous translation model named \acrfull{RCTM} \cite{kalchbrenner_recurrent_2013}
%Early \acrshort{NMT} research originates from \cite{sutskever_sequence_2014} and \cite{kalchbrenner_recurrent_2013}.
% The advantage of a sequence to sequence model is that it is possible to map a variable-length input sequence into a variable-length output sequence.
These models are trained using sequence to sequence learning, where it is possible to map a variable-length input sequence into a variable-length output sequence. %, something that was not possible with conventional \acrshort{DNN}s.
Early research of sequence to sequence neural network models derive from \cite{sutskever_sequence_2014}. They proposed a sequence to sequence solution using the \acrshort{LSTM} architecture that can be simplified into two distinct stages, commonly referred to as an encoder-decoder model:
\begin{itemize}
    \item Encode the input sequence using an \acrshort{LSTM} to create a fixed-length vector
    \item Decode the output sequence from the fixed-length vector using another \acrshort{LSTM}
    %\item Use one LSTM to read the input sequence, one step at a time, to obtain large fixed-dimensional vector representation
    %\item Use another LSTM to extract the output sequence from that vector
\end{itemize}

The \acrshort{LSTM} sequence to sequence implementation by \cite{sutskever_sequence_2014} achieved a \acrshort{BLEU} score of $34.8$ on an English to French data set, outperforming a baseline phrased-based \acrshort{SMT} system by $1.5$ \acrshort{BLEU} with the same training corpus.
A generalisation of the encoder-decoder architecture can be visualised in Figure \ref{fig:encoder_decoder}.
\begin{figure}[ht!]
\centering
\includegraphics[width=0.64\textwidth]{media/literature/machine_translation/mt_encoder-decoder.png}
\caption[Diagram of the encoder-decoder architecture]{The encoder-decoder architecture (\cite{cho_properties_2014})}
\label{fig:encoder_decoder}
\end{figure}

% looks to map sequences to sequences
% sequence learning method that makes minimal assumptions on the sentence structure
% they use multilayered LSTM to map the input sequence to a vector of a fixed dimensionality
% then use another deep LSTM to decode the target sequence from the vector

% english to french translation on WMT-14 dataset achieved translation of 34.8 BLEU score
% LSTM did not have difficulty on long sentences
% phrase based SMT achieves 33.3 on same dataset

%used the \acrfull{LSTM} architecture in a sequence to sequence translation model. 

% use a LSTM to solve sequence to sequence problems
% LSTM to read input sequence, one time-step at a time
% obtain large fixed-dimensional vector representation
% use another LSTM to extract the output sequence from that vector

% the second LSTM is essentially an RNN language model except that it is conditioned on the input sequence
% LSTM's ability to successfully learn make it natural choice for this application


% Talk about encoder-decoder framework
When analysing the properties of \acrshort{NMT} encoder-decoder approaches, \cite{cho_properties_2014} discovered that despite achieving good translation performance on short sentences, when the length of a sentence increases, \acrshort{NMT} translation performance significantly reduces. This is a result of using fixed-length vectors, as longer sentences will struggle to fit within a fixed-length vector without losing certain information, structure, and meaning.

%looks to overcome issues outlined by Cho
It is possible to reduce the likelihood of poor translation quality for long sentences in an encoder-decoder framework by aligning a sequence of vectors with the positions that have highest concentration of relevant information (\cite{bahdanau_neural_2016}). 
Instead of encoding the entire sentence into one fixed-length vector, each sentence is encoded into a series of vectors. A subset of these vectors are automatically selected during decoding based on previous words and the positions of the relevant information, determining the prediction of the output sequence. This known as an attention mechanism, due to the ability of the decoder to select which sections of the source sentence to pay attention to during each part of the output sequence. Results of encoder-decoder attention model show significant improvement over conventional \acrshort{NMT} encoder-decoder models, particularly for long sentences (\cite{luong_effective_2015}). Therefore, the implementation of the translation models used for experimentation of different transfer learning methods in Chapter 3 will include an attention layer.


% most research uses the encoder decoder approach with fixed-length vectors
% this may make it difficult for the neural network to cope with long sentences
% especially if it is longer than those in the training corpus (shown by Cho et al. 2014b)

% this paper introduces an extension to encoder-decoder which learns to align and translate jointly
% each time the model generates a word in a translation, it soft-searches
% for a set of positions in a source sentence where the most relevant info is concentrated
% model then predicts a target word based on context vectors associated with these source positions
% and all previously generated words

% results show proposed approach of jointly learning to align significantly improves translation
% performance over basic encoder-decoder approach
% improvement is obvious with long sentences, but also present in any length






%\cite{wu_googles_2016} google's NMT


% ========================== REMOVED ========================== 

% the encoder processes a variable-length input (source sentence)
% and builds a fixed-length vector representation
% Conditioned on the encoded representation, the
% decoder generates a variable-length sequence (target sentence)



% ========================== REMOVED ========================== 

%\clearpage

\subsection{Evaluation}

%Say something about human translation evaluation, cite a reference and the state why it is not applicable to MT.
%Although likely to provide a more accurate evaluation of translation quality, it is not possible for professional human translators to handle the high output rate of machine translation techniques during training.
Although likely to provide a more accurate evaluation of translation quality, hiring professional human translators is costly and time consuming, making it incompatible with the high output rate of machine translation during training.
Therefore, automatic evaluation plays a key role in the machine translation process, where it is important that translation models can be evaluated quickly and accurately to speed up the training process.

\acrfull{BLEU} is an automatic machine translation algorithm that is widely regarded as the standard evaluation metric, originating from research by \cite{papineni_bleu_2001}. \acrshort{BLEU} score evaluations are calculated based on the difference between the machine translation output and the translation of a professional human translator. If they are very similar, a high \acrshort{BLEU} score will be awarded. Overall this approach works well, however, translations are scored lower regardless of context or meaning if different words are used. This makes it virtually impossible to achieve a perfect score, even for professional human translators, unless the exact same ordering of words are used. Despite this drawback, it remains the state-of-the-art automatic translation evaluation metric.

% Multiple reference translations
% As long as the translation is pretty close, it will get a high BLEU score
% look at the machine generated output and see if it appears in at least one of the human generated references

% unigram
% precision: what fraction of the mt output appears in the reference translations
% modified precision: out of 7 words, give it 2 credits for appearing a maximum 2
% MAX ( count("the)) / count("the")
% sum of the words that appear over the y output (mt output)
% divided by sum of all unigrams in mt output 

% bigrams to define BLEU score:
% word pairs - count how many each of the pairs appears
% give credit for the maximum number of times a pair appears
% modified bigram precision = sum of count clips / total number of bigrams



% count how many times each of the n-grams appear
% define the clipped count:
% taking all the counts and reducing them to be no more than the number of
% times they appear in a single reference translation

% sum of the count clips divided by the total number of bigrams

The underlying metric of \acrshort{BLEU} is the 'precision measure', which is determined by the fraction of the translation output that appears in the reference translations. This is expanded upon in the 'modified precision measure' which involves the following three steps:
\begin{itemize}
    \item Count the occurrences of each n-gram in the reference translations
    \item Clip (reduce) the counts to be equal to the maximum number of times the n-gram appears in a single reference
    \item Divide the sum of all clipped counts by the total number of n-gram occurrences
\end{itemize}
%counting the maximum number of times a word occurs in any single reference translation
%Next, one clips the total count of each candidate word by its maximum reference count, adds these
%clipped counts up, and divides by the total (unclipped) number of candidate words.

% Stolen -----------------------------------------
%\acrshort{BLEU} score is calculated by taking the geometric mean of modified precision scores and then multiplying the result by an exponential brevity penalty factor.
% Stolen -----------------------------------------
\acrshort{BLEU} score is calculated using the geometric mean of the modified precision scores multiplied by an exponential brevity penalty.
The brevity penalty ($BP$) that is designed to penalise translations that are too short. This adjustment factor helps ensure that a translation with a high \acrshort{BLEU} score not only matches in words and word ordering but in length as well.
If the translation length is more than the reference output length then the brevity penalty is $1$. Otherwise, the brevity penalty is calculated using the following equation, where $r$ is the effective reference corpus length and $c$ is the candidate translation length:
\begin{equation}
    BP = e^{(1-r/c)}
\end{equation}

%\begin{equation}
%    BP = \left \{ \begin{matrix}
%    1 & if  c > r \\ 
%    e^{(1-r/c)} & if  c \leq r
%    \end{matrix}\right.
%\end{equation}

The full equation for BLEU score can be seen below, where $BP$ is the brevity penalty, $N$ is number of n-grams, $w_n$ is the positive weights that total $1$, and $p_n$ is the geometric mean of the modified precision measure:

\begin{equation}
    BLEU = BP \cdot exp\left (  \sum_{n=1}^{N} w_{n} \log  p_{n}\right )
\end{equation}


Variations of BLEU such as \acrshort{BLEU}-$2$, and \acrshort{BLEU}-$3$, and \acrshort{BLEU}-$4$ refer to the cumulative n-gram score. Cumulative n-gram scores are calculated at all orders from $1$ to $n$ and weighted using the geometric mean ($1/n$). For example, \acrshort{BLEU}-$2$ has a geometric mean of $1/2$ so the $1$-gram and $2$-gram score weights are $50$\%.


%The \acrshort{BLEU}-2 weights are 1/2 ($1/n$) for all of the n-gram scores (1-gram, 2-gram). Using the weights to calculate the weighted geometric mean


%and the cumulative n-gram score is derived from the weighted geometric mean from all orders 1 to $n$.

%This is calculated using the individual n-gram scores at all orders from 1 to n and weighting them by calculating the weighted geometric mean.
%The weights for BLEU-4 are 1/4 (25\%) or 0.25 for each of the 1-gram, 2-gram, 3-gram, and 4-gram scores.


%The cumulative n-gram score is derived from the weighted geometric mean from orders 1 to n, where the weight of each 

%The cumulative n-gram score derives from the weighted geometric mean of all orders from 1 to n.



% if mt output is exactly the same as reference 1 or 2 then all of these values will be equal to 1.0

% calculate p1, p2, p3, p4 and combine them together. average of n 1-4 p^n

% BP = brevity penalty
% an adjustment factor that penalises translation systems that outputs translation systems that are too short
% if mt output_length > reference output length then 1
% else exp(1 - mt_output_length / reference_output_length)


\acrshort{BLEU} score has a range of $0$ and $1$, with $1$ being the highest translation score possible. A score of $1$ indicates that it is a direct copy of the reference translation, making it difficult to achieve for human translators, even if their translation is still valid. To improve the readability of translation performance results, \acrshort{BLEU} score is typically referenced as a percentage rather than a small number between $1$ and $0$. For example, $45$ \acrshort{BLEU} score represents $0.45$ \acrshort{BLEU}. In terms of \acrshort{BLEU} score translation interpretability, scores over $30$ are typically understandable and scores that are higher than $50$ indicate a fluent translation (\cite{lavie_evaluating_2010}).

\cite{papineni_bleu_2001} conducted experiments for translations in a variety of languages where \acrshort{BLEU} scores were compared with the judgements of both monolingual native English speakers and bilingual English speakers in order to determine the accuracy of the automatic evaluation. Results showed a significantly high correlation between \acrshort{BLEU} score and human translation score evaluations.


% There are a number of automatic translation evaluation algorithms that each benefit differently in various contexts.
% \acrfull{METEOR} automatic translation evaluation technique that

%\acrfull{METEOR} is another automatic translation evaluation technique that  


Although \acrshort{BLEU} is the most popular automatic evaluation metric, there are alternatives that are also capable of machine translation evaluation. \acrfull{METEOR} is a metric proposed by \cite{banerjee_meteor_2005} that was designed to overcome some of the weaknesses of \acrshort{BLEU}. For example, \cite{banerjee_meteor_2005} states because \acrshort{BLEU} does not require word-to-word matching, n-gram counts of matches between the translation and reference translations can be incorrect. In contrast, \acrshort{METEOR} does evaluate n-gram counts based on explicit word-to-word matches. 
%In a comparative case study of \acrshort{BLEU} and \acrshort{METEOR}, \cite{laith_comparative_2015} concluded that \acrshort{BLEU} has a higher correlation to human evaluation. (THE CASE STUDY FOCUS WAS ON ARABIC, METEOR HAD NOT FULLY SUPPORTED ARABIC YET...)

%\cite{denkowski_meteor_2014} Meteor translation technique. apparently better than BLEU

%\begin{itemize}
%    \item This section is not finished:
%    \item Explain METEOR Universal translation evaluation technique (\cite{denkowski_meteor_2014})
%    \item Compare METEOR and BLEU (\cite{laith_comparative_2015}) % BLEU is better apparently
%\end{itemize}




% ===================== REMOVED ===================== 

%Research by \cite{wolk_comparison_2017} compares \acrshort{BLEU}, EBLEU, NIST, and METEOR with 

%the human translation evaluation model NER. % not true. its not human
% comparison between BLEU and others. BLEU wins again. Most similar to human scores

% measuring accuracy of the data has to take semantic meaning into account,
% rather than just blindly performing simple word-to-word comparison

% NER is a human translation evaluation method. commonly used in re-speaking
% it is expensive and time-consuming [8]
% human effort cant keep up with the growing demand for evaluation

% BLEU is the most significant predictor of NER
% NIST is significant
% EBLEU is also significant

% ===================== REMOVED ===================== 





%\clearpage


%\section{Corpus Augmentation}
\section{Low-Resource Machine Translation}
\label{sec:2-low_resource_mt}
% Add a paragraph that provides an overview for low resource MT

\subsection{Back-Translation}
%Back-translation makes it possible to generate synthetic parallel data by pairing monolingual data with a back-translated version of the data.
Although monolingual data can be used to improve the performance of phrase-based \acrfull{SMT} using the language model, this is not the case for \acrshort{NMT}, where neural models are trained using parallel training data. Modern back-translation typically works by using \acrshort{NMT} to train a model that translates backwards from the target language to the source language. Once the model is trained, it is used to translate the monolingual data and create a synthetic parallel corpus. This can be seen in Figure \ref{fig:back_trans}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.55\textwidth]{media/literature/data_argumentation/da_back_trans.png}
\caption[Diagram of the back-translation synthetic parallel corpus]{Back-translation synthetic parallel corpus creation (\cite{hoang_iterative_2018})}
\label{fig:back_trans}
\end{figure}
%<<<<< There are various techniques, this is by \cite{hoang_iterative_2018} iterative back translation>>>>>
Research by \cite{sennrich_improving_2016} incorporates monolingual data into \acrshort{NMT} by studying the effect of using back-translation on monolingual data in order to improve translation models.  Without any alterations to the underlying architecture, their findings indicate that adding the synthetic data to the training corpus significantly improved translation quality by $3.4$ BLEU score in both high-resource and low-resource training data sets.



% back-translation of monolingual target data into the source language,
% and treating this synthetic data as additional training data

% back translation is simple and easy to do as it doesn't require modification to MT training algorithms
% instead requires target to source system into order to generate additional synthetic parallel data
% from monolingual target data

% pairing monolingual training data with automatic back-translation,
% treat it as additional parallel training data
% Improvements on the low-resourced IWSLT 14 task Turkish > English (2.1 - 3.4 BLEU)
% state of the art results (in 2015)

% introduce back-translation technique for NMT
% shows the quality of the back-translation model, and therefore resulting pseudo-corpus
% has a positive effect on the quality of the subsequent source-to-target model

%\cite{graca_generalizing_2019} problems with sampling-based approaches + remedies
% current state of the art NMT models suffer from smearing issues (ott et al 2018)
% and are trained using label smoothing (pereyra et al, 2017)
% these result in low quality sampled sentences

% they investigate considering only high quality hypotheses by restricting search space of the model via:
% - ignoring words under a probability threshold during sampling
% - N-best list sampling

% Experiments show their restricted sampling techniques are comparable to or better than other generation methods
% by imitating human-generated data better

% translation quality is still not with consistent improvements over typical beam search strategy


%\cite{edunov_back_trans_2018} understanding back-translation at scale
% broadens understanding of back-translation
% investigates methods to generate synthetic source sentences
% scale to hundreds of millions of monolingual sentences and achieve new state of the art
% they found in low-resource settings, back-translation obtained by sampling or noised beam outputs
% are the most effective
% noise beam is sentences with various types of noise (deleting, replacing and swapping words)


\subsection{Sentence Segmentation}

Sentence segmentation is the process of using punctuation marks within a sentence as delimiters to divide the sentence into multiple partial sentences. When applied to an existing parallel corpus that contains long sentences with punctuation, sentence segmentation can be used as a data augmentation technique. \cite{zhang_corpus_2019} implemented this by generating pseudo-parallel sentence pairs using sentence segmentation with back-translation as follows:
\begin{itemize}
    \item Divide the sentences in a parallel training data set into partial sentences
    \item Back-translate the partial sentences from the target language
    \item Use back-translated data to replace partial sentences from the source language
\end{itemize}

Results of their sentence segmentation implementation demonstrate that the increase of parallel sentence pairs can lead to improvements over baseline \acrshort{NMT} translation performance. In addition, their proposed method outperformed models using the back-translation augmentation method for the Japanese - Chinese 'ASPEC-JC' (\cite{NAKAZAWA16.621}) training corpus.

% Segmenting long sentences in a corpus using back-translation
% and generating pseudo-parallel sentence pairs
% improves translation performance




%\cite{kuang_automatic_2016}
% automatic long sentence -- improves translation for long sentences...
% nothing to do with training data augmentation

%\cite{fadaee_data_2017}
% replace certain words with other words. not sentence segmentation....
% Targets low-frequency words by generating new sentence pairs, containing new rare words
% in synthetically created contexts
% improves translation quality up to 2.9 BLEU over baseline
% and up to 3.2 over back-translation

\subsection{Easy Data Augmentation}
\acrfull{EDA} is a data augmentation technique which aims to improve \acrshort{NLP} text classification performance by creating augmented training data to artificially increase the size of the corpus. It is a corpus augmentation technique that uses a combination of word replacements, insertions, swaps, and deletions. Additional parameters such as number of augmented sentences per original sentence, and the percentage of words from the original sentence to change allow for fine-tuning of the output relevant to the usage context. For individual sentences in the training data, an augmented sentence is generated using an operation selected randomly from four different techniques:
\begin{itemize}
    \item \textbf{Synonym Replacement:} Select $n$ words at random and replace each one with a synonym
    \item \textbf{Random Insertion:} Insert the synonym of any word into any position. Repeat $n$ times
    \item \textbf{Random Swap:} Swap the position of any two words. Repeat $n$ times
    \item \textbf{Random Deletion:} Randomly delete each word with probability $p$
\end{itemize}

\cite{wei_eda:_2019} found that \acrshort{EDA} increased performance for both recurrent and convolutional neural networks and improvements are most significant when the data was restricted to simulate a low-resource scenario. The additional training data generated and noise from the variety of swaps contribute towards reduced overfitting. In a text classification task, \acrshort{EDA} can achieve the same level of accuracy as the baseline performance of the entire training corpus despite only using only $50$\% of the training corpus. However, \acrshort{EDA} experiments have focussed exclusively on its application to text classification. Although it may be useful for generating additional monolingual data, \acrshort{EDA} cannot be applied to a parallel data set consisting of two different languages due to the replacement that occurs without the use of a language model.

% say something about overfitting and how the context remains similar


% improves performance for convolutional and recurrent neural networks
% strong results for small data sets
% on average, only took 50% of the available training data to achieve the same accuracy as full data set

% contextual augmentation, noising, GAN or back-translation may work better, but:
% high cost of implementing relative to the expected performance gain
% EDA is a set of simple techniques that are generalisable to a range of NLP tasks

% added noise from random swaps can help prevent overfitting
% synonym is more likely to be relevant to the context and retain the original label of the sentence


%\begin{figure}[ht!]
%\centering
%\includegraphics[width=0.45\textwidth]{media/literature/data_argumentation/eda_class.png}
%\caption[\acrshort{EDA} augmented data latent space visualisation]{EDA comparison of original and augmented sentences in a %latent space visualisation (\cite{wei_eda:_2019})
%}
%\label{fig:transfer}
%\end{figure}
\subsection{Contextual Data Augmentation}

Contextual data augmentation is a type of data augmentation where words are replaced at random using predictions from a language model, based on the context of the word within the sentence. As with other augmentation techniques, the primary aim is to reduce overfitting and improve generalisation of the models that train on the augmented data. 

Although capable of retaining contextual information, contextual data augmentation research is primarily focussed on text classification tasks rather than \acrshort{NMT}. Research by \cite{wu_conditional_2018} and \cite{kobayashi_contextual_2018} are good examples of this, where the augmented data can be fairly similar to the original data making it significantly less beneficial for \acrshort{NMT} training despite remaining useful in \acrshort{NLP} classifiers. This is difficult to overcome due to limitations in the usage of vocabulary without repeating the augmentation process many times for each sentence while maintaining grammatically correct output.

\acrfull{SCDA} is a method of data augmentation proposed by \cite{zhu_soft_2019}, specifically designed for use in \acrshort{NMT} systems. The \acrshort{SCDA} uses a language model that is trained on the same training corpus as the \acrshort{NMT} model, as shown in Figure \ref{fig:scda}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.58\textwidth]{media/literature/data_argumentation/da_scda.png}
\caption[Diagram of the \acrlong{SCDA} encoder architecture]{\acrlong{SCDA} encoder architecture (\cite{zhu_soft_2019})}
\label{fig:scda}
\end{figure}

The key difference is that random words from the original sentences are replaced with a mix of contextually related words using a probability distribution vector.




Their findings demonstrate that the \acrshort{SCDA} method provides a consistent improvement of more than $1.0$ BLEU score for transformer model \acrshort{NMT} in comparison to alternative approach baselines using a transformer model with both small and large data sets.

% Data augmentation is good in deep learning computer vision,
% it is still limited in natural language

% this paper presents a novel data augmentation method for NMT
% different from previous methods that drop swap or replace words with other words

% other methods often need to augment one sentence multiple times and replace
% a different subset of words in the original sentence with different candidate words in vocabulary
% which still cannot guarantee adequate variations of augmented sentences 

% they softly augment a random word in a sentence by its contextual mixture of multiple related words
% by replacing the one-hot representation of a word by a distribution over the vocabulary

% they use a distribution vector to replace a randomly chosen word from the original sentence

% results demonstrate superiority over baselines for small and large scale MT data sets
% consistently achieve more than 1.0 BLEU score improvement over strong Transformer base system for all tasks
% unlike other methods that may not work well in all tasks,
% theirs does, regardless of the data set

% The augmented data closely match the position of the original data,


\section{Low-Resource Machine Translation Approaches}
\label{LRNMT}


\subsection{Existing Scottish Gaelic Machine Translation}
%Existing research on Scottish Gaelic machine translation has focussed on using \acrshort{SMT}

Research by \cite{dowling_leveraging_2019} takes advantage of the increased data availability of a high-resource language (Irish Gaelic) and uses back-translation to create a parallel corpus with Scottish Gaelic, a closely related low-resource language pair. As shown in Figure \ref{fig:lang_pair}, the sentence structure of Irish Gaelic (GD) and Scottish Gaelic (GA) is very similar, making it an ideal choice for back-translation.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.4\textwidth]{media/literature/nmt_approaches/lr_gaelic.png}
\caption[Diagram of the similarities in a closely related language pair]{Similarities in a closely related language pair (\cite{dowling_leveraging_2019})}
\label{fig:lang_pair}
\end{figure}


The \acrshort{SMT} model saw improvements in performance over baseline when combining the synthetic training data with the original training data. The reason stated for not using \acrshort{NMT} in this research was due to the limited corpus size. As \acrshort{NMT} translation quality suffers significantly when models are trained with a low quantity of data, and as demonstrated in research by \cite{dowling_smt_2018}, a well tailored \acrshort{SMT} model achieves much better translation quality in comparison to an "out-of-the-box" \acrshort{NMT} model for Irish translation. Therefore, the research of low-resource neural machine translation for Scottish Gaelic may contribute towards solving this problem. This project aims to expand upon the existing research and explore the application of \acrshort{NMT} to Scottish Gaelic through the implementation of transfer learning techniques.

\subsection{Transfer Learning}
\label{sec:2-transfer_learning}
As outlined by \cite{torrey_transfer_2009}, transfer learning uses the knowledge gained from a previous task in order to improve model performance in a related task. This concept is illustrated in Figure \ref{fig:transfer}, where knowledge gained from the source domain $A$ is used to help inform the target domain $B$.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\textwidth]{media/literature/nmt_approaches/transfer.png}
\caption[Diagram of the transfer learning process]{The process of transfer learning (\cite{ruder_transfer_2019})}
\label{fig:transfer}
\end{figure}


In a neural machine translation context, this involves training a model with data from a high-resource language and then using that model to initialise the weights of the model that will be trained on the low-resource language.
This was demonstrated in research by \cite{zoph_transfer_2016}, where transfer learning improved the performance of \acrshort{NMT} models for low-resource languages by an average of $5.6$ \acrshort{BLEU} on four different language pairs. Results also suggest that selecting a high-resource language closely related to the low-resource language can improve transfer learning models and therefore translation quality.

However, this contradicts more recent research by \cite{kocmi_trivial_2018} which looks at "trivial transfer learning". Existing transfer learning methods require a degree of language relatedness, whereas trivial transfer learning prioritises data quantity for the high-resource language. Their findings indicate that the relatedness of the language pair is of less importance than the quantity of data used in the initial high-resource language training. Despite being unable to pinpoint the exact reasoning behind the improvement in results, they state that "our observations indicate that the key factor is the size of the parent corpus rather than e.g. vocabulary overlaps".
It is worth noting that \cite{kocmi_trivial_2018} use a transformer neural network instead of the recurrent neural network used by \cite{zoph_transfer_2016}. Research by \cite{popel_training_2018} found that using the transformer model leads to better translation quality, likely contributing towards the contradictory results.


Hierarchical transfer learning seeks ensure the closeness of the related language pair, as identified in most transfer learning research, while simultaneously addressing the importance of the high-resource data quantity outlined in trivial transfer learning. \cite{luo_hierarchical_2019} achieve this by implementing three distinct stages of training:

\begin{itemize}
  \item Train the model using an unrelated high-resource language pair
  \item Initialise the next model and train on an intermediate language pair
  \item Initialise the final model and train using the low-resource language pair
\end{itemize}
% The first stage involves training using an unrelated high-resource language pair. Once the model has reached convergence, it is used to initialise the next model that will be trained on an intermediate language pair, closely related to the low-resource language. Upon convergence, the final model in the hierarchy is initialised and the low-resource language pair is trained.

Results indicate improvements of up to $0.58$ \acrshort{BLEU} score in comparison to the aforementioned transfer learning methods that are limited to a parent-child architecture.


% Train the NMT model using the unrelated high-resource language pair
% Train using the similar intermediate language pair
% Train using the low-resource language pair


\subsection{Meta Learning}
% Seems like an extension of hierarchical transfer learning

% what is meta learning
Meta learning can be thought of as the machine learning process of "learning how to learn". Observing the performance of different approaches on a variety of tasks and then using this experience to influence the learning process of new tasks in order to considerably increase the rate of learning (\cite{vanschoren_meta-learning:_2018}).
% talk about model agnostic meta learning
\acrfull{MAML} is a meta learning algorithm proposed by \cite{finn_model-agnostic_2017}, where models are trained to adapt quickly. This leads to good a generalisation performance on a new task despite a low quantity of training data.
A diagram of the \acrshort{MAML} algorithm can be seen in Figure \ref{fig:MAML}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.40\textwidth]{media/literature/nmt_approaches/maml.png}
\caption[Diagram of a \gls{MAML} algorithm]{An illustration of the \Gls{MAML} algorithm (\cite{finn_model-agnostic_2017})}
\label{fig:MAML}
\end{figure}

Despite the primary focus of \acrshort{MAML} research relating to object recognition, it can be applied to a variety of machine learning problems with any number of training steps or training data because the algorithm still produces a weight initialisation, meaning no additional learning parameters are required.

% as shown in Figure ~\ref{fig:MAML}.
% goal of meta learning is to train model on variety of tasks 
% so that it can solve new tasks w/ small number of training samples

% In MAML, parameters of the model are trained such that a small number of gradient steps
% with a small amount of training data from a new task will produce good generalisation performance on that task

% Essentially, trains the model to be easy to fine-tune 
%
% Leads to state-of-the-art-performance with few-shot image classification benchmarks
%

% say how this paper uses meta agnostic model for translation
Research by \cite{gu_meta-learning_2018} is the first of its kind to use \acrshort{MAML} for \acrshort{NMT}. In comparison to the transfer based approach by \cite{zoph_transfer_2016}, results showed further improvements with a BLEU score of $22.04$, despite training data for the low-resource language limited to $16,000$ words (around $600$ parallel sentences). As the corpus size of the low-resource language decreases, transfer learning approaches suffer significantly more than meta learning, which proves the effectiveness of \acrshort{MAML} for low-resource languages. However, as the corpus size increases, the differences in BLEU score between the two approaches are much less significant.

\section{Conclusions}

This chapter reviewed the literature surrounding neural networks and machine translation, leading to approaches for low-resource neural machine translation. 

From the research covered in this review, it is clear that there have been many important developments in machine translation since its origin. Advancements in statistical machine translation and more recently the shift towards neural machine translation with convolutional neural networks and recurrent neural networks have led to significant improvements in translation quality.

As confirmed by the literature, various low-resource \acrshort{NMT} techniques have shown to achieve significantly higher translation quality scores than baseline \acrshort{NMT} approaches on the same low-resource data corpus. Transfer learning has been the main focus of low-resource neural machine translation research, however, new research on meta learning has shown promising results with improvements over transfer learning. A variety of automatic evaluation metrics that can be used for evaluating the performance of translation models were identified in the research. \acrshort{BLEU} score will be the most beneficial metric to use during training and evaluation due to its high correlation to human translators and widespread adoption among virtually all other machine translation research.

Limitations of recent research primarily involves the scope of each implementation. There are many techniques and implementation decisions that have shown to improve translation quality, however, the majority of the research goes into great detail about one particular choice. Understandably, individual papers have a clear focus, but there is an identifiable gap in the research regarding the impact of using a combination of the aforementioned techniques. Data augmentation techniques such as back-translation have shown to improve \acrshort{NMT} quality on baseline \acrshort{NMT} approaches, so it is worth investigating what the impact when used with low-resource \acrshort{NMT} approaches. There is also no existing research for Scottish Gaelic \acrshort{NMT} systems as prior research was limited to statistical machine translation due to the large quantity of training data required with generic \acrshort{NMT} approaches. Transfer learning has shown promising results in other low-resource languages, so it may be a good alternative for Scottish Gaelic.

% Expand on why you used the approaches you used and what are their benefits.

Based on the review of literature, the low-resource \acrshort{NMT} techniques that this project implements are trivial transfer learning and hierarchical transfer learning. The initialisation of a neural network with the weights of a previously trained neural network in a similar domain helps to aid the translation quality, particularly in a low-resource context. Experimentation in regards to the relatedness of the language pair, particularly Irish Gaelic which also has a very similar sentence structure, will identify whether either the quantity or relatedness of data is of a higher importance for Scottish Gaelic.


% purpose of this chapter (lit review)
% from research it is clear that (mention some important developments)
% as confirmed by literature, <low resource translation techniques> have shown to be successful for other lr languages
% any limitations of the research
% finally i will investigate these approaches and analyse 