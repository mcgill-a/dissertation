\begin{savequote}[75mm]
Add your quote here
\qauthor{Author of quote}
\end{savequote}

\chapter{Literature Review}
\label{literature}
%\newthought{Lorem ipsum dolor sit amet}, consectetur adipiscing elit. Nunc neque massa, eleifend malesuada enim eu, efficitur imperdiet nulla. Aliquam augue est, lobortis eget scelerisque at, pretium vel lorem. Etiam faucibus nulla quam, nec tristique elit ullamcorper quis. Phasellus vel tincidunt nisl. Mauris at lacus sit amet mi auctor feugiat. Donec vel maximus erat. Suspendisse feugiat augue lacus, non egestas libero auctor sit amet. Nunc eros ante, posuere quis justo id, congue ultrices sapien.

\clearpage


\section{Machine Learning}




\begin{itemize}
    \item Backpropagation + vanishing gradient problem
    \item Recurrent Neural Networks (RNN)
    \item Long Short Term Memory (LSTM)
    \item Transformer model
\end{itemize}

\acrfull{LSTM} is an \acrshort{RNN} architecture proposed by \cite{hochreiter_long_1997} that was designed to help overcome the vanishing gradient problem present during backpropagation, using a gradient-based algorithm that enforces a consistent internal state error flow. This ensures that gradients will not become insignificant and halt the learning process.

\subsection{Neural Networks}

\subsection{Deep Learning}

\subsection{Keras}



\clearpage
\section{Machine Translation}
\label{Machine Translation}
\subsection{Available Datasets}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Language(s)}} & \textbf{Data Type} & \multicolumn{1}{c|}{\textbf{Source}} \\ \hline
\begin{tabular}[c]{@{}l@{}}Scottish Gaelic (GD)\\ English (EN)\end{tabular} & Bilingual & OPUS: Open Parallel Corpus \\ \hline
\begin{tabular}[c]{@{}l@{}}Scottish Gaelic (GD)\\ English (EN)\end{tabular} & Bilingual & LearnGaelic PDF Learning Materials \\ \hline
\begin{tabular}[c]{@{}l@{}}Scottish Gaelic (GD)\\ English (EN)\end{tabular} & Bilingual & Scottish Parliament Publications \\ \hline
Placeholder & Monolingual & Placeholder \\ \hline
\end{tabular}%
}
\end{table}

\clearpage
\subsection{Techniques}

\subsubsection{Statistical Machine Translation}

% every sentence in one language is a possible translation of any sentence in another
% Assign every pair of sentences a probability
% Pr(T|S) - probability that a translator will produce T in the
%           target language when presented with S in the source language

% View the problem of machine translation as follows:
% given a sentence T in the target language,
% Seek the sentence S from which the translator produced T.
% Chance of error is minimised by choosing that sentence S that is most probable given T
% Thus, we wish to choose S so as to maximise Pr(S|T)

\acrfull{SMT} is a statistical approach for machine translation first presented by \cite{brown_statistical_1990}. 
\acrshort{SMT} assumes that all sentences in a source language have the possibility of being the correct translation of a sentence in a target language. 
In other words, find the source sentence S that the translator used to produce the target sentence T by selecting the pair with the highest probability $Pr ( T | S )$.

This is done using a language model, translation model, and decoder
as shown below in Figure \ref{fig:smt_diagram}.

\begin{figure}[ht!]
\centering
\includegraphics[width=1\textwidth]{media/literature/machine_translation/smt_3.png}
\caption[Diagram of \acrshort{SMT} probability distribution and decoder]{Left side: \acrshort{SMT} probability distribution for source (S) and target (T) sentence pairs. Right side: \acrshort{SMT} decoder selecting the highest probability sentence pair (\cite{brown_statistical_1990})}
\label{fig:smt_diagram}
\end{figure}



% SMT - instead of defining all the language rules and exceptions to rules, let the system find the rules itself
% analyse millions of parallel data sets that have already been translated by human translators
% once the computer finds the pattern, use this pattern to translate text in the future
% repeat this process a lot, lots of patterns, lots of translations


% probabilistic models estimated from parallel corpora
% need large quantities of training data


% From SLIDES: http://folk.uio.no/plison/pdfs/talks/meetup_SMT.pdf
% encodes the faithfulness of T as a translation of S

% early translation models relied on probabilities for individual words
% did not account well for idiosyncratic expressions

% decompose translation probability P(s|t)  into phrase pairs
% - phrase probability (as given by the translation table)
% - distortion probability (relative distance between the phrase positions in the two languages)

% Three challenges:
% estimating the language model probability, Pr(e) - called the language modelling problem
% estimating the translation model probability, Pr(f|e) - called the translation modelling problem
% devising an effective and efficient sub-optimal search for the English string that maximises their product
% ^ called the search problem search problem

%Explain the language model:
The language model provides an estimation of how probable a given source sentence is based on the probability distribution of word sequences. This model is associated with the fluency of the translation as it is trained using target language monolingual data, providing the guidelines for well written translation output. Using a large training corpus, the n-gram probability of every word is determined from the words immediately proceeding it. One drawback to this approach is that the unrecognised increased probability for words that are dependant on each other but are further apart.

%in scenarios where words are linked to other words that are further away from them, 

%n-gram probabilities on large amounts of monolingual data 

%This is accomplished by using large amounts of monolingual data, n-gram probabilities can 



% bigram or general m-gram model

% encodes the fluency of T in the target language

% also want the translated sentence T to be fluent in the target language
% statistical language model is a probability distribution over sequences of words
% typically represented as n-grams, where the probability of each word depends on the words occurring before it

% N-gram probabilities can be estimated from large amounts of monolingual data
% Bigrams or trigrams are most popular
% Smoothing methods to account for data sparsity

% Does not do well with long-range dependencies (words that depend on another word that is multiple words away)

%Explain the translation model:
% decomposed into a lexical and an alignment model
% estimation of the lexical correspondence between languages (lexical = words or vocab of a language)

%The translation model proposed by \cite{brown_statistical_1990} includes the following parameters:
The translation model is an estimation of the source and target language vocabulary correspondence. It is associated with the quality of the translations, as it is responsible for predicting the translations of words and short sequences of words, mapping the source language to the target language. The model parameters proposed by are estimated by training probabilistic models on large quantities of parallel training data, derived from the following sets of probabilities as proposed by \cite{brown_statistical_1990}:
\begin{itemize}
    \item Fertility probabilities:  $Pr(n|s)$ - the probability that a source word $s$ generates $n$ target words in a given source-target sentence alignment
    
    \item Lexical probabilities: $Pr(s|t)$ - the probability that a source word $s$ translates into a target word $t$, for each element in the source and target language vocabularies
    
    \item Distortion probabilities: $Pr(i|j, l)$ - the probability of the position of a target word $i$, based on the position of the source word $j$, and the length of the target sentence $l$
    
    %\item Probability p1: p(you|NULL) the probability that the spurious word you is generated from NULL
    % %\item if an English word isn't aligned with a french word then it is omitted
    %\item Lexical probability t: t(Quan|When) the probability that Quan translates into When
    
    %\item Distortion d: d(j|i, m, n) the probability that the word in position j generates a word in position i. m and n are the length of the source and target sentences
    % \item distortions allow adjectives to precede the noun that they modify in english but to follow them in french
\end{itemize}


% \cite{brown_mathematics_1993}
% the language model probability is large for well-formed English strings regardless of their connection to French.
% together, they produce a large probability for well-formed English strings that account well for French.

% alignment mapping i j -> i = a_j from source position j to target position i = a_j
% use of alignment model raises major problems as it fails to capture dependencies between groups of words
% it is difficult to handle different word order and the translation of compound nouns

% \cite{och_1999_improved} described two methods for SMT that extend the baseline alignment model
% to account for these problems


%Explain the decoder

During the runtime of an \acrshort{SMT} system, the decoder uses the translation and language models to find the best translation from the source input to the target language output.

The decoder starts off with an empty hypothesis for the translation. The hypothesis is expanded incrementally by partial hypotheses using the translation and language models. As there are an exponential number of hypotheses in relation to the length of the source sentence, search optimisation techniques are required to find the most likely translation. A beam search is a very efficient decoding technique that can be used to confine the search space to a limited quantity of low cost hypotheses by comparing hypotheses with equal length translation output and removing those with a high cost and estimated future cost (\cite{koehn_pharaoh_2004}).

% how to use translation and language models to find the best translation T for a sentence S?

% Search through the space of possible translations
% incremental decoding process (beam search): gradual expansion of translation hypothesis
% every partial hypothesis is associated with a cost
% translation and language models + estimate of future costs
% beam search only keeps track of a limited number of good hypotheses (based on their cost), the rest is discarded

% the search space is further reduced through hypothesis recombination


%\cite{brown_statistical_1990}
%\begin{itemize}
%    \item SMT first introduced
%    \item Most successful phrase-based method to translate input text
%    \item Translate sequences of words at a time using a parallel corpus
%\end{itemize}


%\cite{schwenk_continuous_2012}
%\begin{itemize}
%    \item Phrase-based SMT further improvements
%\end{itemize}

% introduced by IBM 1990, 1993
% most successful use phrase-based method to translate input text
% by translating sequences of words at a time

% phrase based use a language model trained from a parallel corpus
% translation model in broken down into several components:
% reordering model, phrase translation model, world translation model

\clearpage
\subsubsection{Neural Machine Translation}

\acrfull{NMT} is a modern approach to machine translation that uses neural networks to generate statistical models capable of translating sentences from a source language into sentences in the target language.
% mention its wide spread adoption and state of the art results
% used by companies such as Google for Google Translate
% Bahdanau has a background section on NMT:
% in NMT, we fit a parameterized model to maximise the conditional probability of sentence pairs
% using a parallel training corpus
% Once the conditional distribution is learned by a translation model
% given a source sentence a corresponding translation can be generated by searching for
% the sentence that matches the conditional probability
%<<introduce sequence to sequence models, leading onto the next section>>
%continuous translation model named \acrfull{RCTM} \cite{kalchbrenner_recurrent_2013}
%Early \acrshort{NMT} research originates from \cite{sutskever_sequence_2014} and \cite{kalchbrenner_recurrent_2013}.
% The advantage of a sequence to sequence model is that it is possible to map a variable-length input sequence into a variable-length output sequence.
These models are trained using sequence to sequence learning, where it is possible to map a variable-length input sequence into a variable-length output sequence. %, something that was not possible with conventional \acrshort{DNN}s.
Early research of sequence to sequence neural network models derive from \cite{sutskever_sequence_2014}. They proposed a sequence to sequence solution using the \acrshort{LSTM} architecture that can be simplified into two distinct stages, commonly referred to as an encoder-decoder model:
\begin{itemize}
    \item Encode the input sequence using an \acrshort{LSTM} to create a fixed-length vector
    \item Decode the output sequence from the fixed-length vector using another \acrshort{LSTM}
    %\item Use one LSTM to read the input sequence, one step at a time, to obtain large fixed-dimensional vector representation
    %\item Use another LSTM to extract the output sequence from that vector
\end{itemize}

The \acrshort{LSTM} sequence to sequence implementation by \cite{sutskever_sequence_2014} achieved a \acrshort{BLEU} score of 34.8 on an English to French data set, outperforming a baseline phrased-based \acrshort{SMT} system by 1.5 \acrshort{BLEU} with the same training corpus.
A generalisation of the encoder-decoder architecture can be visualised below in Figure \ref{fig:encoder_decoder}.
\begin{figure}[ht!]
\centering
\includegraphics[width=0.64\textwidth]{media/literature/machine_translation/mt_encoder-decoder.png}
\caption[Diagram of the encoder-decoder architecture]{The encoder-decoder architecture (\cite{cho_properties_2014})}
\label{fig:encoder_decoder}
\end{figure}

% looks to map sequences to sequences
% sequence learning method that makes minimal assumptions on the sentence structure
% they use multilayered LSTM to map the input sequence to a vector of a fixed dimensionality
% then use another deep LSTM to decode the target sequence from the vector

% english to french translation on WMT-14 dataset achieved translation of 34.8 BLEU score
% LSTM did not have difficulty on long sentences
% phrase based SMT achieves 33.3 on same dataset

%used the \acrfull{LSTM} architecture in a sequence to sequence translation model. 

% use a LSTM to solve sequence to sequence problems
% LSTM to read input sequence, one time-step at a time
% obtain large fixed-dimensional vector representation
% use another LSTM to extract the output sequence from that vector

% the second LSTM is essentially an RNN language model except that it is conditioned on the input sequence
% LSTM's ability to successfully learn make it natural choice for this application


% Talk about encoder-decoder framework
When analysing the properties of \acrshort{NMT} encoder-decoder approaches, \cite{cho_properties_2014} discovered that despite achieving good translation performance on short sentences, when the length of a sentence increases, \acrshort{NMT} translation performance significantly reduces. This is a result of using fixed-length vectors, as longer sentences will struggle to fit within a fixed-length vector without losing certain information, structure, and meaning.

%looks to overcome issues outlined by Cho
It is possible to reduce the likelihood of poor translation quality for long sentences in an encoder-decoder framework by aligning a sequence of vectors with the positions that have highest concentration of relevant information (\cite{bahdanau_neural_2016}). 
Rather than encoding the entire sentence into one fixed-length vector, the input sentence is encoded multiple vectors. A subset of these vectors are automatically selected during decoding based on previous words and the positions of the relevant information, determining the prediction of the output sequence sentence. This known as an attention mechanism, due to the ability of the decoder to select which sections of the source sentence to pay attention to during each part of the output sequence. Results of encoder-decoder attention model show significant improvement over conventional \acrshort{NMT} encoder-decoder models, particularly for long sentences.


% most research uses the encoder decoder approach with fixed-length vectors
% this may make it difficult for the neural network to cope with long sentences
% especially if it is longer than those in the training corpus (shown by Cho et al. 2014b)

% this paper introduces an extension to encoder-decoder which learns to align and translate jointly
% each time the model generates a word in a translation, it soft-searches
% for a set of positions in a source sentence where the most relevant info is concentrated
% model then predicts a target word based on context vectors associated with these source positions
% and all previously generated words

% results show proposed approach of jointly learning to align significantly improves translation
% performance over basic encoder-decoder approach
% improvement is obvious with long sentences, but also present in any length






%\cite{wu_googles_2016} google's NMT


% ========================== REMOVED ========================== 

% the encoder processes a variable-length input (source sentence)
% and builds a fixed-length vector representation
% Conditioned on the encoded representation, the
% decoder generates a variable-length sequence (target sentence)



% ========================== REMOVED ========================== 

\clearpage
\subsection{Evaluation}

%Say something about human translation evaluation, cite a reference and the state why it is not applicable to MT.
Although likely to provide a more accurate evaluation of translation quality, it is not possible for professional human translators to handle the high output rate of machine translation techniques during training. Therefore, automatic evaluation plays a key role in the machine translation process, where it is important that translation models can be evaluated quickly and accurately to speed up the training process.

\acrfull{BLEU} is an automatic machine translation algorithm that is widely regarded as the standard evaluation metric, originating from research by \cite{papineni_bleu_2001}. \acrshort{BLEU} score evaluations are calculated based on the difference between the machine translation output and the translation of a professional human translator. If they are very similar, a high \acrshort{BLEU} score will be awarded. Overall this approach works well, however, translations are scored lower regardless of context or meaning if different words are used. This makes it virtually impossible to achieve a perfect score, even for professional human translators, unless the exact same ordering of words are used. Despite this drawback, it remains to be a state-of-the-art automatic translation evaluation algorithm.

% Multiple reference translations
% As long as the translation is pretty close, it will get a high BLEU score
% look at the machine generated output and see if it appears in at least one of the human generated references

% unigram
% precision: what fraction of the mt output appears in the reference translations
% modified precision: out of 7 words, give it 2 credits for appearing a maximum 2
% MAX ( count("the)) / count("the")
% sum of the words that appear over the y output (mt output)
% divided by sum of all unigrams in mt output 

% bigrams to define BLEU score:
% word pairs - count how many each of the pairs appears
% give credit for the maximum number of times a pair appears
% modified bigram precision = sum of count clips / total number of bigrams



% count how many times each of the n-grams appear
% define the clipped count:
% taking all the counts and reducing them to be no more than the number of
% times they appear in a single reference translation

% sum of the count clips divided by the total number of bigrams

The underlying metric of \acrshort{BLEU} is the 'precision measure', which is determined by the fraction of the translation output that appears in the reference translations. This is expanded upon in the 'modified precision measure' which involves the following steps:
\begin{itemize}
    \item Count the occurrences of each n-gram in the reference translations
    \item Clip (reduce) the counts to be no more than then number of times the n-gram appears in a single reference
    \item Divide the sum of all clipped counts by the total number of n-gram occurrences
\end{itemize}
%counting the maximum number of times a word occurs in any single reference translation
%Next, one clips the total count of each candidate word by its maximum reference count, adds these
%clipped counts up, and divides by the total (unclipped) number of candidate words.

% Stolen -----------------------------------------
\acrshort{BLEU} score is calculated by taking the geometric mean of modified precision scores and then multiply the result by an exponential brevity penalty factor
% Stolen -----------------------------------------

\acrshort{BLEU} also has a brevity penalty ($BP$) that is designed to penalise translations that are too short. This adjustment factor helps ensure that a translation with a high \acrshort{BLEU} score not only matches in words and word ordering but in length as well.
If the translation length is more than the reference output length then then brevity penalty is 1. Otherwise, the brevity penalty is calculated using the following equation, where $r$ is the effective reference corpus length and $c$ is the candidate translation length:
\begin{equation}
    BP = e^{(1-r/c)}
\end{equation}

%\begin{equation}
%    BP = \left \{ \begin{matrix}
%    1 & if  c > r \\ 
%    e^{(1-r/c)} & if  c \leq r
%    \end{matrix}\right.
%\end{equation}

The full equation for BLEU score can be seen below, where $BP$ is the brevity penalty, $N$ is number of n-grams, $w_n$ is the  positive weights that total 1, and $p_n$ is the geometric mean of the modified precision measure:

\begin{equation}
    BLEU = BP \cdot exp\left (  \sum_{n=1}^{N} w_{n} \log  p_{n}\right )
\end{equation}


Variations of BLEU such as \acrshort{BLEU}-2, and \acrshort{BLEU}-3, and \acrshort{BLEU}-4 refer to the cumulative n-gram score. Cumulative n-gram scores are calculated at all orders from 1 to n and weighted using the geometric mean ($1/n$). For example, \acrshort{BLEU}-2 has a geometric mean of $1/2$ so the 1-gram and 2-gram score weights are 50\%.


%The \acrshort{BLEU}-2 weights are 1/2 ($1/n$) for all of the n-gram scores (1-gram, 2-gram). Using the weights to calculate the weighted geometric mean


%and the cumulative n-gram score is derived from the weighted geometric mean from all orders 1 to $n$.

%This is calculated using the individual n-gram scores at all orders from 1 to n and weighting them by calculating the weighted geometric mean.
%The weights for BLEU-4 are 1/4 (25\%) or 0.25 for each of the 1-gram, 2-gram, 3-gram, and 4-gram scores.


%The cumulative n-gram score is derived from the weighted geometric mean from orders 1 to n, where the weight of each 

%The cumulative n-gram score derives from the weighted geometric mean of all orders from 1 to n.



% if mt output is exactly the same as reference 1 or 2 then all of these values will be equal to 1.0

% calculate p1, p2, p3, p4 and combine them together. average of n 1-4 p^n

% BP = brevity penalty
% an adjustment factor that penalises translation systems that outputs translation systems that are too short
% if mt output_length > reference output length then 1
% else exp(1 - mt_output_length / reference_output_length)


\acrshort{BLEU} score has a range of 0 and 1, with 1 being the highest translation score possible. A score of 1 indicates that it is a direct copy of the reference translation, making it difficult to achieve for human translators, even if their translation is still valid. To improve the readability of translation performance results, \acrshort{BLEU} score is typically referenced as a percentage rather than a small number between 1 and 0. For example, 45 \acrshort{BLEU} score represents 0.45 \acrshort{BLEU}. In terms of \acrshort{BLEU} score translation interpretability, scores over 30 are typically understandable and scores that are higher than 50 indicate a fluent translation (\cite{lavie_evaluating_2010}).

\cite{papineni_bleu_2001} conducted experiments for translations in a variety of languages where \acrshort{BLEU} scores were compared with the judgements of both monolingual native English speakers and bilingual English speakers in order to determine the accuracy of the automatic evaluation. Results showed a significantly high correlation between \acrshort{BLEU} score and human translation score evaluations.


% There are a number of automatic translation evaluation algorithms that each benefit differently in various contexts.
% \acrfull{METEOR} automatic translation evaluation technique that

\acrfull{METEOR} is another automatic translation evaluation technique that  


Possible next part of this section:
\begin{itemize}
    \item Explain METEOR translation evaluation technique (\cite{denkowski_meteor_2014})
    \item Compare METEOR and BLEU. BLEU is better apparently (\cite{laith_comparative_2015})
\end{itemize}

%\cite{denkowski_meteor_2014} Meteor translation technique. apparently better than BLEU..
%\cite{laith_comparative_2015} comparison between BLEU and Meteor. BLEU wins

% ===================== REMOVED ===================== 

%Research by \cite{wolk_comparison_2017} compares \acrshort{BLEU}, EBLEU, NIST, and METEOR with 

%the human translation evaluation model NER. % not true. its not human
% comparison between BLEU and others. BLEU wins again. Most similar to human scores

% measuring accuracy of the data has to take semantic meaning into account,
% rather than just blindly performing simple word-to-word comparison

% NER is a human translation evaluation method. commonly used in re-speaking
% it is expensive and time-consuming [8]
% human effort cant keep up with the growing demand for evaluation

% BLEU is the most significant predictor of NER
% NIST is significant
% EBLEU is also significant

% ===================== REMOVED ===================== 






%\cite{callison_bleu_2006} BLEU maybe not so great

\clearpage

% Should this be under machine translation section?
\section{Corpus Augmentation}

\subsection{Back-Translation}
%Back-translation makes it possible to generate synthetic parallel data by pairing monolingual data with a back-translated version of the data.
Although monolingual data can be used to improve the performance of phrase-based \acrfull{SMT} using the language model, this is not the case for \acrshort{NMT}, where neural models are trained using parallel training data. Modern back-translation typically works by using \acrshort{NMT} to train a model that translates backwards from the target language to the source language. Once the model is trained, it is used to translate the monolingual data and create a synthetic parallel corpus. This can be visualised below in Figure \ref{fig:back_trans}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.55\textwidth]{media/literature/data_argumentation/da_back_trans.png}
\caption[Diagram of the back-translation synthetic parallel corpus]{Back-translation synthetic parallel corpus creation (\cite{hoang_iterative_2018})}
\label{fig:back_trans}
\end{figure}
%<<<<< There are various techniques, this is by \cite{hoang_iterative_2018} iterative back translation>>>>>
Research by \cite{sennrich_improving_2016} incorporates monolingual data into \acrshort{NMT} by studying the effect of using back-translation on monolingual data in order to improve translation models.  Without any alterations to the underlying architecture, their findings indicate that adding the synthetic data to the training corpus significantly improved translation quality by 3.4 BLEU score in both high-resource and low-resource training data sets.



% back-translation of monolingual target data into the source language,
% and treating this synthetic data as additional training data

% back translation is simple and easy to do as it doesn't require modification to MT training algorithms
% instead requires target to source system into order to generate additional synthetic parallel data
% from monolingual target data

% pairing monolingual training data with automatic back-translation,
% treat it as additional parallel training data
% Improvements on the low-resourced IWSLT 14 task Turkish > English (2.1 - 3.4 BLEU)
% state of the art results (in 2015)

% introduce back-translation technique for NMT
% shows the quality of the back-translation model, and therefore resulting pseudo-corpus
% has a positive effect on the quality of the subsequent source-to-target model

%\cite{graca_generalizing_2019} problems with sampling-based approaches + remedies
% current state of the art NMT models suffer from smearing issues (ott et al 2018)
% and are trained using label smoothing (pereyra et al, 2017)
% these result in low quality sampled sentences

% they investigate considering only high quality hypotheses by restricting search space of the model via:
% - ignoring words under a probability threshold during sampling
% - N-best list sampling

% Experiments show their restricted sampling techniques are comparable to or better than other generation methods
% by imitating human-generated data better

% translation quality is still not with consistent improvements over typical beam search strategy


%\cite{edunov_back_trans_2018} understanding back-translation at scale
% broadens understanding of back-translation
% investigates methods to generate synthetic source sentences
% scale to hundreds of millions of monolingual sentences and achieve new state of the art
% they found in low-resource settings, back-translation obtained by sampling or noised beam outputs
% are the most effective
% noise beam is sentences with various types of noise (deleting, replacing and swapping words)


\subsection{Sentence Segmentation}

Sentence segmentation is the process of using punctuation marks within a sentence as delimiters to divide the sentence into multiple partial sentences. When applied to an existing parallel corpus that contains long sentences with punctuation, sentence segmentation can be used as a data augmentation technique. \cite{zhang_corpus_2019} implemented this by generating pseudo-parallel sentence pairs using sentence segmentation with back-translation as follows:
\begin{itemize}
    \item Divide the sentences in a parallel training data set into partial sentences
    \item Back-translate the partial sentences from the target language
    \item Use back-translated data to replace partial sentences from the source language
\end{itemize}

Results of their sentence segmentation implementation demonstrate that the increase of parallel sentence pairs can lead to improvements over baseline \acrshort{NMT} translation performance. In addition, their proposed method outperformed models using the back-translation augmentation method for the Japanese - Chinese 'ASPEC-JC' (\cite{NAKAZAWA16.621}) training corpus.

% Segmenting long sentences in a corpus using back-translation
% and generating pseudo-parallel sentence pairs
% improves translation performance




%\cite{kuang_automatic_2016}
% automatic long sentence -- improves translation for long sentences...
% nothing to do with training data augmentation

%\cite{fadaee_data_2017}
% replace certain words with other words. not sentence segmentation....
% Targets low-frequency words by generating new sentence pairs, containing new rare words
% in synthetically created contexts
% improves translation quality up to 2.9 BLEU over baseline
% and up to 3.2 over back-translation

\subsection{Easy Data Augmentation}
\acrfull{EDA} is a data augmentation technique which aims to improve \acrfull{NLP} text classification performance by creating augmented training data to artificially increase the size of the corpus. It is the corpus augmentation technique that uses a combination of word replacements, insertions, swaps, and deletions. Additional input parameters such as number of augmented sentences per original sentence, and the percentage of words from the original sentence to change allow for fine-tuning of the output relevant to the usage context. For each individual sentence in the training data, an augmented sentence is generated using an operation selected randomly from four different techniques:
\begin{itemize}
    \item \textbf{Synonym Replacement:} Select \textit{n} words at random and replace each one with a synonym
    \item \textbf{Random Insertion:} Insert the synonym of any word into any position. Repeat \textit{n} times
    \item \textbf{Random Swap:} Swap the position of any two words. Repeat \textit{n} times
    \item \textbf{Random Deletion:} Randomly delete each word with probability \textit{p}
\end{itemize}

\cite{wei_eda:_2019} found that \acrshort{EDA} increased model performance for both recurrent and convolutional neural networks and improvements are most significant when the data set was restricted to simulate a low-resource scenario. The additional training data generated and noise from the variety of swaps contribute towards reduced overfitting. In a text classification task, \acrshort{EDA} can achieve the same level of accuracy as the baseline performance of the entire training corpus despite only using only 50\% of the training corpus. Thus far, experiments of \acrshort{EDA} have focussed exclusively on its application to text classification. Therefore, it is unclear whether its benefits will be applicable in \acrshort{NMT} and is worth investigating further.

% say something about overfitting and how the context remains similar


% improves performance for convolutional and recurrent neural networks
% strong results for small data sets
% on average, only took 50% of the available training data to achieve the same accuracy as full data set

% contextual augmentation, noising, GAN or back-translation may work better, but:
% high cost of implementing relative to the expected performance gain
% EDA is a set of simple techniques that are generalisable to a range of NLP tasks

% added noise from random swaps can help prevent overfitting
% synonym is more likely to be relevant to the context and retain the original label of the sentence


%\begin{figure}[ht!]
%\centering
%\includegraphics[width=0.45\textwidth]{media/literature/data_argumentation/eda_class.png}
%\caption[\acrshort{EDA} augmented data latent space visualisation]{EDA comparison of original and augmented sentences in a %latent space visualisation (\cite{wei_eda:_2019})
%}
%\label{fig:transfer}
%\end{figure}
\subsection{Contextual Data Augmentation}

Contextual data augmentation is a type of data augmentation where words are replaced at random using predictions from a language model, based on the context of the word within the sentence. As with other augmentation techniques, the primary aim is to reduce overfitting and improve generalisation of the models that train on the augmented data. 

Although capable of retaining contextual information, contextual data augmentation research is primarily focussed on text classification tasks rather than \acrshort{NMT}. Research by \cite{wu_conditional_2018} and \cite{kobayashi_contextual_2018} are good examples of this, where the augmented data is can be fairly similar to the original data making it significantly less beneficial for \acrshort{NMT} training despite remaining useful in \acrshort{NLP} classifiers. This is difficult to overcome due to limitations in the usage of vocabulary without repeating the augmentation process many times for each sentence while maintaining grammatically correct output.

\acrfull{SCDA} is a method of data augmentation proposed by \cite{zhu_soft_2019}, specifically designed for use in \acrshort{NMT} systems. The \acrshort{SCDA} uses a language model that is trained on the same training corpus as the \acrshort{NMT} model. As shown below in Figure \ref{fig:scda}, the key difference is that random words from the original sentences are replaced with a mix of contextually related words using a probability distribution vector.


\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\textwidth]{media/literature/data_argumentation/da_scda.png}
\caption[Diagram of the \acrlong{SCDA} encoder architecture]{\acrlong{SCDA} encoder architecture (\cite{zhu_soft_2019})}
\label{fig:scda}
\end{figure}

Their findings demonstrate that the \acrshort{SCDA} method provides a consistent improvement of more than 1.0 BLEU score for transformer model \acrshort{NMT} in comparison to alternative approach baselines using a transformer model with both small and large data sets.

% Data augmentation is good in deep learning computer vision,
% it is still limited in natural language

% this paper presents a novel data augmentation method for NMT
% different from previous methods that drop swap or replace words with other words

% other methods often need to augment one sentence multiple times and replace
% a different subset of words in the original sentence with different candidate words in vocabulary
% which still cannot guarantee adequate variations of augmented sentences 

% they softly augment a random word in a sentence by its contextual mixture of multiple related words
% by replacing the one-hot representation of a word by a distribution over the vocabulary

% they use a distribution vector to replace a randomly chosen word from the original sentence

% results demonstrate superiority over baselines for small and large scale MT data sets
% consistently achieve more than 1.0 BLEU score improvement over strong Transformer base system for all tasks
% unlike other methods that may not work well in all tasks,
% theirs does, regardless of the data set

% The augmented data closely match the position of the original data,


\section{Low-Resource Machine Translation Approaches}
\label{LRNMT}


\subsection{Existing Scottish Gaelic Machine Translation}
%Existing research on Scottish Gaelic machine translation has focussed on using \acrshort{SMT}

Research by \cite{dowling_leveraging_2019} takes advantage of the increased data availability of a high-resource language (Irish Gaelic) and uses back-translation to create a parallel corpus with Scottish Gaelic, a closely related low-resource language pair. As shown in Figure \ref{fig:lang_pair}, the sentence structure of Irish Gaelic (GD) and Scottish Gaelic (GA) is very similar, making it an ideal choice for back-translation.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.4\textwidth]{media/literature/nmt_approaches/lr_gaelic.png}
\caption[Diagram of the similarities in a closely related language pair]{Similarities in a closely related language pair (\cite{dowling_leveraging_2019})}
\label{fig:lang_pair}
\end{figure}


The \acrshort{SMT} model saw improvements in performance over baseline when combining the synthetic training data with the original training data. The reason stated for not using \acrshort{NMT} in this research was due to the limited corpus size. As \acrshort{NMT} translation quality suffers significantly when models are trained with a low quantity of data, and as demonstrated in research by \cite{dowling_smt_2018}, a well tailored \acrshort{SMT} model achieves much better translation quality in comparison to an "out-of-the-box" \acrshort{NMT} model for Irish translation. Therefore, the research and implementation of low-resource machine translation approaches for Scottish Gaelic may contribute towards solving this problem.

\subsection{Transfer Learning}

As outlined by \cite{torrey_transfer_2009}, transfer learning uses the knowledge gained from a previous task in order to improve model performance in a related task. This concept is illustrated below in Figure \ref{fig:transfer}, where knowledge gained from the source domain Model A is used to help inform the target domain Model B.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{media/literature/nmt_approaches/transfer.png}
\caption[Diagram of the transfer learning process]{The process of transfer learning (\cite{ruder_transfer_2019})}
\label{fig:transfer}
\end{figure}


In a neural machine translation context, this involves training a model with data from a high-resource language and then using that model to initialise the weights of the model the will be trained on the low-resource language.
This was demonstrated in research carried out by \cite{zoph_transfer_2016}, where transfer learning improved the performance of \acrshort{NMT} models for low-resource languages by an average of 5.6 \acrshort{BLEU} on four different language pairs. Results of the experiment also suggest that selecting a high-resource language closely related to the low-resource language can improve transfer learning models and therefore translation quality.

However, this contradicts more recent research by \cite{kocmi_trivial_2018} which looks at "trivial transfer learning". Existing transfer learning methods require a degree of language relatedness, whereas trivial transfer learning prioritises data quantity for the high-resource language. Their findings indicate that the relatedness of the language pair is of less importance than the quantity of data used in the initial high-resource language training. Despite being unable to pinpoint the exact reasoning behind the improvement in results, they state that "our observations indicate that the key factor is the size of the parent corpus rather than e.g. vocabulary overlaps".
It is worth noting that \cite{kocmi_trivial_2018} use a transformer neural network architecture instead of the recurrent neural network architecture used by \cite{zoph_transfer_2016}. Research by \cite{popel_training_2018} found that using the transformer model leads to better translation quality, likely contributing towards the contradictory results.


Hierarchical transfer learning seeks ensure the closeness of the related language pair, as identified in most transfer learning research, while simultaneously addressing the importance of the high-resource data quantity outlined in trivial transfer learning. \cite{luo_hierarchical_2019} achieve this by implementing three distinct stages of training:

\begin{itemize}
  \item Train the model using an unrelated high-resource language pair
  \item Initialise the next model and train on an intermediate language pair
  \item Initialise the final model and train using the low-resource language pair
\end{itemize}
% The first stage involves training using an unrelated high-resource language pair. Once the model has reached convergence, it is used to initialise the next model that will be trained on an intermediate language pair, closely related to the low-resource language. Upon convergence, the final model in the hierarchy is initialised and the low-resource language pair is trained.

Results indicate improvements of up to 0.58 \acrshort{BLEU} score in comparison to the aforementioned transfer learning methods that are limited to a parent-child architecture.


% Train the NMT model using the unrelated high-resource language pair
% Train using the similar intermediate language pair
% Train using the low-resource language pair


\subsection{Meta Learning}
% Seems like an extension of hierarchical transfer learning

% what is meta learning
Meta learning can be thought of as the machine learning process of "learning how to learn". Observing the performance of different approaches on a variety of tasks and then using this experience to influence the learning process of new tasks in order to considerably increase the rate of learning (\cite{vanschoren_meta-learning:_2018}).

% talk about model agnostic meta learning

\acrfull{MAML} is a meta learning algorithm proposed by \cite{finn_model-agnostic_2017}, where models are trained to adapt quickly. This leads to good a generalisation performance on a new task despite a low quantity of training data.
A diagram of the \acrshort{MAML} algorithm can be seen below in Figure \ref{fig:MAML}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.45\textwidth]{media/literature/nmt_approaches/maml.png}
\caption[Diagram of a \gls{MAML} algorithm]{An illustration of the \Gls{MAML} algorithm (\cite{finn_model-agnostic_2017})}
\label{fig:MAML}
\end{figure}

Despite the primary focus of \acrshort{MAML} research relating to object recognition, it can be applied to a variety of machine learning problems with any number of training steps or training data because the algorithm still produces a weight initialisation, meaning no additional learning parameters are required.

% as shown below in Figure ~\ref{fig:MAML}.
% goal of meta learning is to train model on variety of tasks 
% so that it can solve new tasks w/ small number of training samples

% In MAML, parameters of the model are trained such that a small number of gradient steps
% with a small amount of training data from a new task will produce good generalisation performance on that task

% Essentially, trains the model to be easy to fine-tune 
%
% Leads to state-of-the-art-performance with few-shot image classification benchmarks
%

% say how this paper uses meta agnostic model for translation
Research by \cite{gu_meta-learning_2018} is the first of its kind to use \acrshort{MAML} for \acrshort{NMT}. In comparison to the transfer based approach by \cite{zoph_transfer_2016}, results showed further improvements with a BLEU score of 22.04, despite training data for the low-resource language limited to 16,000 words (around 600 parallel sentences). As the corpus size of the low-resource language decreases, transfer learning approaches suffer significantly more than meta learning, which proves the effectiveness of \acrshort{MAML} for low-resource languages. However, as the corpus size increases, the differences in BLEU score between the two approaches are much less significant.

\section{Conclusion}
